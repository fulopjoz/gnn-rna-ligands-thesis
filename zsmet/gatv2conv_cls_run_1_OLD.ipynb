{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36854829",
   "metadata": {
    "papermill": {
     "duration": 0.00928,
     "end_time": "2024-03-15T07:43:34.296491",
     "exception": false,
     "start_time": "2024-03-15T07:43:34.287211",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Graph Neural Network for Molecular Interaction Prediction\n",
    "\n",
    "This Jupyter Notebook outlines the process for training a Graph Neural Network (GNN) model to predict molecular interactions using the GATv2 architecture. The goal of this project is to leverage the inherent graph structure of molecules for effective prediction of binding to RNA, a crucial factor in drug discovery and biological research.\n",
    "\n",
    "Each run of this notebook represents a distinct experiment with specified hyperparameters and configurations. Results and models from each run are saved separately for comparative analysis to ensure the reproducibility and statistical significance of our findings.\n",
    "\n",
    "### Notebook Details:\n",
    "\n",
    "- **Objective**: Predict molecular interactions with RNA using GNN.\n",
    "- **Model Architecture**: GATv2Conv from the Deep Graph Library (DGL).\n",
    "- **Data Source**: Preprocessed molecular interaction datasets.\n",
    "- **Run Number**: This notebook facilitates multiple runs. Specific details for each run, including the random state and run number, are set at the beginning to ensure reproducibility.\n",
    "\n",
    "Before executing the notebook, please adjust the `RANDOM_STATE` and `RUN_NUMBER` variables at the top of the notebook to reflect the specific experiment being conducted. This setup ensures each run's outputs are unique and traceable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b64e986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T07:43:34.317506Z",
     "iopub.status.busy": "2024-03-15T07:43:34.316939Z",
     "iopub.status.idle": "2024-03-15T07:43:34.338767Z",
     "shell.execute_reply": "2024-03-15T07:43:34.337088Z"
    },
    "papermill": {
     "duration": 0.037716,
     "end_time": "2024-03-15T07:43:34.342683",
     "exception": false,
     "start_time": "2024-03-15T07:43:34.304967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis script implements a Graph Neural Network (GNN) using the GATv2 architecture\\nfor the purpose of predicting molecular interactions. The implementation leverages\\nthe Deep Graph Library (DGL) for constructing and manipulating graphs, as well as\\nOptuna for hyperparameter optimization. The model includes features such as dropout,\\nearly stopping, and gradient scaling for improved training stability and performance.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script implements a Graph Neural Network (GNN) using the GATv2 architecture\n",
    "for the purpose of predicting molecular interactions. The implementation leverages\n",
    "the Deep Graph Library (DGL) for constructing and manipulating graphs, as well as\n",
    "Optuna for hyperparameter optimization. The model includes features such as dropout,\n",
    "early stopping, and gradient scaling for improved training stability and performance.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35353ac1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T07:43:34.361293Z",
     "iopub.status.busy": "2024-03-15T07:43:34.360675Z",
     "iopub.status.idle": "2024-03-15T07:43:36.292614Z",
     "shell.execute_reply": "2024-03-15T07:43:36.290149Z"
    },
    "papermill": {
     "duration": 1.95209,
     "end_time": "2024-03-15T07:43:36.303130",
     "exception": false,
     "start_time": "2024-03-15T07:43:34.351040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the random seed and run number at the top for reproducibility and to differentiate runs\n",
    "RANDOM_STATE = 420  # Change for each run if needed\n",
    "RUN_NUMBER = 1  # Change for each run\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import dgl\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "dgl.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd79759",
   "metadata": {
    "papermill": {
     "duration": 0.017365,
     "end_time": "2024-03-15T07:43:36.330190",
     "exception": false,
     "start_time": "2024-03-15T07:43:36.312825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1634e7b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T07:43:36.351682Z",
     "iopub.status.busy": "2024-03-15T07:43:36.349980Z",
     "iopub.status.idle": "2024-03-15T07:43:37.220664Z",
     "shell.execute_reply": "2024-03-15T07:43:37.218982Z"
    },
    "papermill": {
     "duration": 0.885813,
     "end_time": "2024-03-15T07:43:37.224902",
     "exception": false,
     "start_time": "2024-03-15T07:43:36.339089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from dgl.nn import GATv2Conv, GlobalAttentionPooling\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from torch.cuda.amp import GradScaler, autocast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a782b481",
   "metadata": {
    "papermill": {
     "duration": 0.00829,
     "end_time": "2024-03-15T07:43:37.243135",
     "exception": false,
     "start_time": "2024-03-15T07:43:37.234845",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The Graph Neural Network Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a5e9945",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T07:43:37.261738Z",
     "iopub.status.busy": "2024-03-15T07:43:37.261099Z",
     "iopub.status.idle": "2024-03-15T07:43:37.271444Z",
     "shell.execute_reply": "2024-03-15T07:43:37.269962Z"
    },
    "papermill": {
     "duration": 0.02363,
     "end_time": "2024-03-15T07:43:37.274844",
     "exception": false,
     "start_time": "2024-03-15T07:43:37.251214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphClsGATv2(nn.Module):\n",
    "    \"\"\"\n",
    "    A Graph Neural Network (GNN) model using the GATv2 architecture for graph \n",
    "    classification.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_feats : int\n",
    "        The number of input features.\n",
    "    hidden_dim : int\n",
    "        The number of output features.\n",
    "    num_heads : int\n",
    "        The number of attention heads.\n",
    "    feat_drop : float\n",
    "        The dropout rate for the input features.\n",
    "    attn_drop : float\n",
    "        The dropout rate for the attention scores.\n",
    "    negative_slope : float\n",
    "        The negative slope for the LeakyReLU activation function.\n",
    "    num_cls : int\n",
    "        The number of classes for classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_feats, hidden_dim, num_heads, feat_drop, attn_drop, \n",
    "                 negative_slope, num_cls):\n",
    "        super(GraphClsGATv2, self).__init__()\n",
    "        self.gatv2_1 = GATv2Conv(in_feats,\n",
    "                                 hidden_dim,\n",
    "                                 num_heads,\n",
    "                                 feat_drop=feat_drop,\n",
    "                                 attn_drop=attn_drop,\n",
    "                                 negative_slope=negative_slope,\n",
    "                                 activation=F.elu,\n",
    "                                 residual=True)\n",
    "        # The second GATv2 layer\n",
    "        self.gatv2_2 = GATv2Conv((hidden_dim * num_heads),\n",
    "                                 hidden_dim,  \n",
    "                                 1,  # Single head in the second layer\n",
    "                                 feat_drop=feat_drop,\n",
    "                                 attn_drop=attn_drop,\n",
    "                                 negative_slope=negative_slope,\n",
    "                                 activation=F.elu,\n",
    "                                 residual=True)\n",
    "        # The global attention pooling layer\n",
    "        self.pooling = GlobalAttentionPooling(nn.Linear(hidden_dim, 1))\n",
    "        # The fully connected layer for classification\n",
    "        self.fc = nn.Linear(hidden_dim, num_cls)\n",
    "        self.dropout = nn.Dropout(feat_drop)\n",
    "\n",
    "    def forward(self, graph, feat):\n",
    "        # Apply the first GATv2 layer\n",
    "        h = self.gatv2_1(graph, feat)\n",
    "        # reshape the feature tensor to have the same shape as the input tensor\n",
    "        h = h.reshape(h.shape[0], -1)\n",
    "        h = self.dropout(h)\n",
    "        # No need to flatten since DGL handles dimensionality appropriately\n",
    "        h = self.gatv2_2(graph, h)\n",
    "        # Aggregate node features to graph-level features\n",
    "        hg = self.pooling(graph, h).squeeze()  # Global pooling\n",
    "        # Classify based on the graph-level representation\n",
    "        return self.fc(hg)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        for layer in self.children():\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce086d0e",
   "metadata": {
    "papermill": {
     "duration": 0.008152,
     "end_time": "2024-03-15T07:43:37.291213",
     "exception": false,
     "start_time": "2024-03-15T07:43:37.283061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Early Stopping Mechanism\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4f95520",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T07:43:37.308783Z",
     "iopub.status.busy": "2024-03-15T07:43:37.308497Z",
     "iopub.status.idle": "2024-03-15T07:43:37.317968Z",
     "shell.execute_reply": "2024-03-15T07:43:37.316597Z"
    },
    "papermill": {
     "duration": 0.020941,
     "end_time": "2024-03-15T07:43:37.320253",
     "exception": false,
     "start_time": "2024-03-15T07:43:37.299312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation\n",
    "    loss doesn't improve after a given patience.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    patience : int\n",
    "        How long to wait after last time validation loss improved.\n",
    "    verbose : bool\n",
    "        If True, prints a message for each validation loss improvement.\n",
    "    delta : float\n",
    "        Minimum change in the monitored quantity to qualify as an improvement.\n",
    "    path : str\n",
    "        The file path where the model will be saved.\n",
    "    print_freq : int\n",
    "        The frequency at which to print messages during training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            patience=7,\n",
    "            verbose=True,\n",
    "            delta=0.001,\n",
    "            path='checkpoint.pt',\n",
    "            print_freq=5):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.best_epoch = 0\n",
    "        self.print_freq = print_freq\n",
    "        \n",
    "    def __call__(self, val_loss, model, epoch):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.best_epoch = epoch\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose and self.counter % self.print_freq == 0:\n",
    "                print(\n",
    "                    f'EarlyStopping counter: {self.counter} '\n",
    "                    f'out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "            self.best_epoch = epoch\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f'Validation loss decreased ({self.val_loss_min:.6f} '\n",
    "                f'--> {val_loss:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5029e6f0",
   "metadata": {
    "papermill": {
     "duration": 0.003684,
     "end_time": "2024-03-15T07:43:37.328460",
     "exception": false,
     "start_time": "2024-03-15T07:43:37.324776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The Collate Function for DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90643514",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T07:43:37.372070Z",
     "iopub.status.busy": "2024-03-15T07:43:37.371791Z",
     "iopub.status.idle": "2024-03-15T07:43:37.377844Z",
     "shell.execute_reply": "2024-03-15T07:43:37.376305Z"
    },
    "papermill": {
     "duration": 0.048637,
     "end_time": "2024-03-15T07:43:37.380695",
     "exception": false,
     "start_time": "2024-03-15T07:43:37.332058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    \"\"\"\n",
    "    Function to collate samples into a batch for the GraphDataLoader.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    samples : list\n",
    "        A list of tuples of the form (graph, label).\n",
    "    \"\"\"\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return batched_graph, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994b3a4f",
   "metadata": {
    "papermill": {
     "duration": 0.006236,
     "end_time": "2024-03-15T07:43:37.393321",
     "exception": false,
     "start_time": "2024-03-15T07:43:37.387085",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training and Evaluation Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "449a5a83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T07:43:37.440012Z",
     "iopub.status.busy": "2024-03-15T07:43:37.439758Z",
     "iopub.status.idle": "2024-03-15T07:43:37.458760Z",
     "shell.execute_reply": "2024-03-15T07:43:37.457336Z"
    },
    "papermill": {
     "duration": 0.062191,
     "end_time": "2024-03-15T07:43:37.461686",
     "exception": false,
     "start_time": "2024-03-15T07:43:37.399495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainingPipeline:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "    def train_and_evaluate(\n",
    "            self,\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            early_stopping,\n",
    "            num_epochs,\n",
    "            plot_curves=False,\n",
    "            accumulation_steps=8):\n",
    "        train_losses, val_losses = [], []\n",
    "        scaler = GradScaler()  # Initialize the gradient scaler\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            optimizer.zero_grad()  # Initialize gradients to zero\n",
    "\n",
    "            for batch_idx, (batched_graph, labels) in enumerate(train_loader):\n",
    "                batched_graph, labels = batched_graph.to(\n",
    "                    self.device), labels.to(self.device)\n",
    "\n",
    "                with autocast():  # Enable automatic mixed precision\n",
    "                    logits = model(\n",
    "                        batched_graph, batched_graph.ndata['h'].float())\n",
    "                    loss = criterion(logits, labels) / \\\n",
    "                        accumulation_steps  # Scale loss\n",
    "\n",
    "                # Scale the loss and call backward to propagate gradients\n",
    "                scaler.scale(loss).backward()\n",
    "                # Correct scaling for logging purposes\n",
    "                train_loss += loss.item() * accumulation_steps\n",
    "\n",
    "                if (batch_idx + 1) % accumulation_steps == 0 or \\\n",
    "                        batch_idx == len(train_loader) - 1:\n",
    "                    # Perform optimizer step using scaled gradients\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()  # Update the scaler for the next iteration\n",
    "                    optimizer.zero_grad()  # Initialize gradients to zero\n",
    "\n",
    "            train_loss /= len(train_loader)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            total = 0\n",
    "            if val_loader is not None:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_correct = 0\n",
    "                    total = 0\n",
    "                    for batched_graph, labels in val_loader:\n",
    "                        batched_graph, labels = batched_graph.to(\n",
    "                            self.device), labels.to(self.device)\n",
    "                        with autocast():  # Enable automatic mixed precision\n",
    "                            logits = model(\n",
    "                                batched_graph, batched_graph.ndata['h'].float()\n",
    "                            )\n",
    "                            loss = criterion(logits, labels)\n",
    "                        val_loss += loss.item()\n",
    "                        _, predicted = torch.max(logits.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "                    val_loss /= len(val_loader)\n",
    "                    val_losses.append(val_loss)\n",
    "                    val_accuracy = val_correct / total\n",
    "\n",
    "                    if early_stopping:\n",
    "                        early_stopping(val_loss, model, epoch + 1)\n",
    "                        if early_stopping.early_stop:\n",
    "                            print(\n",
    "                                f\"Early stopping triggered\"\n",
    "                                f\"at epoch {epoch + 1}\")\n",
    "                            break\n",
    "\n",
    "                if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                    print(\n",
    "                        f'Epoch {epoch + 1}/{num_epochs} - '\n",
    "                        f'Train Loss: {train_loss:.4f}, '\n",
    "                        f'Val Loss: {val_loss:.4f} '\n",
    "                        f'| Val accuracy: {100 * val_accuracy:.2f}%')\n",
    "\n",
    "        if plot_curves and val_loader is not None:\n",
    "            self.plot_loss_curves(train_losses, val_losses)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_loss_curves(train_losses, val_losses):\n",
    "        sns.set(style=\"whitegrid\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "        plt.plot(epochs, train_losses, label='Training Loss')\n",
    "        plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss of GATv2Conv')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'loss_curves_{RUN_NUMBER}.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate_on_test(self, model, test_loader, criterion, run_id):\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_accuracy = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batched_graph, labels in test_loader:\n",
    "                batched_graph, labels = batched_graph.to(self.device), \n",
    "                labels.to(self.device)\n",
    "                logits = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "                loss = criterion(logits, labels)\n",
    "                test_loss += loss.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                test_accuracy += torch.sum(preds == labels).item()\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_accuracy /= len(test_loader.dataset)\n",
    "        \n",
    "        # Calculate ROC-AUC\n",
    "        roc_auc = roc_auc_score(all_labels, all_preds) \n",
    "        \n",
    "        # Calculate and save confusion matrix\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot()\n",
    "        plt.savefig(f'confusion_matrix_{run_id}.png', dpi=300)\n",
    "        \n",
    "        # Append results to CSV including ROC-AUC\n",
    "        results_df = pd.DataFrame({\n",
    "            'Run ID': [run_id],\n",
    "            'Test Loss': [test_loss],\n",
    "            'Test Accuracy': [test_accuracy],\n",
    "            'ROC-AUC': [roc_auc]\n",
    "        })\n",
    "        results_df.to_csv('test_results.csv', mode='a', index=False, \n",
    "                          header=not os.path.exists('test_results.csv'))\n",
    "        \n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy}\")\n",
    "        print(f\"ROC-AUC: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558eb08d",
   "metadata": {
    "papermill": {
     "duration": 0.063521,
     "end_time": "2024-03-15T07:43:37.531177",
     "exception": false,
     "start_time": "2024-03-15T07:43:37.467656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hyperparameter Optimization Using Optuna\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3c9f0e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T07:43:37.544993Z",
     "iopub.status.busy": "2024-03-15T07:43:37.544722Z",
     "iopub.status.idle": "2024-03-15T07:43:37.558378Z",
     "shell.execute_reply": "2024-03-15T07:43:37.557112Z"
    },
    "papermill": {
     "duration": 0.023684,
     "end_time": "2024-03-15T07:43:37.561222",
     "exception": false,
     "start_time": "2024-03-15T07:43:37.537538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HyperparameterOptimizer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            device,\n",
    "            subset_train_graphs,\n",
    "            subset_train_labels,\n",
    "            subset_val_graphs,\n",
    "            subset_val_labels,\n",
    "            num_trials,\n",
    "            num_epochs):\n",
    "        self.device = device\n",
    "        self.subset_train_graphs = subset_train_graphs\n",
    "        self.subset_train_labels = subset_train_labels\n",
    "        self.subset_val_graphs = subset_val_graphs\n",
    "        self.subset_val_labels = subset_val_labels\n",
    "        self.num_trials = num_trials\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def objective(self, trial):\n",
    "        # Adjusting the hyperparameters for GATv2Conv\n",
    "        in_feats = 74  \n",
    "        hidden_dim = trial.suggest_int('hidden_dim', 16, 256)\n",
    "        num_heads = trial.suggest_categorical('num_heads', [1, 2, 3, 4, 5, \n",
    "                                                              6, 7, 8, 9, 10, \n",
    "                                                            12, 14, 16, 18, 20])\n",
    "        feat_drop = trial.suggest_float('feat_drop', 0.0, 0.5)\n",
    "        attn_drop = trial.suggest_float('attn_drop', 0.0, 0.5)\n",
    "        negative_slope = trial.suggest_float('negative_slope', 0.01, 0.2)\n",
    "        lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128,\n",
    "                                                                256, 512])\n",
    "\n",
    "        # Create the model, optimizer, and loaders\n",
    "        model = GraphClsGATv2(\n",
    "            in_feats=in_feats,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            feat_drop=feat_drop,\n",
    "            attn_drop=attn_drop,\n",
    "            negative_slope=negative_slope,\n",
    "            num_cls=2,\n",
    "        ).to(self.device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        train_loader = GraphDataLoader(\n",
    "            list(zip(self.subset_train_graphs, self.subset_train_labels)),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate,\n",
    "            num_workers=3)\n",
    "        val_loader = GraphDataLoader(\n",
    "            list(zip(self.subset_val_graphs, self.subset_val_labels)),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=collate,\n",
    "            num_workers=3)\n",
    "\n",
    "        # Training loop with pruning\n",
    "        model.train()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for batched_graph, labels in train_loader:\n",
    "                batched_graph, labels = batched_graph.to(\n",
    "                    self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "                loss = criterion(logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Validation phase and report for pruning\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batched_graph, labels in val_loader:\n",
    "                    batched_graph, labels = batched_graph.to(\n",
    "                        self.device), labels.to(self.device)\n",
    "                    logits = model(\n",
    "                        batched_graph, batched_graph.ndata['h'].float())\n",
    "                    loss = criterion(logits, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            # Report intermediate value to the pruner\n",
    "            trial.report(val_loss, epoch)\n",
    "\n",
    "            if trial.should_prune():  # Handle pruning based on the \n",
    "                                      # intermediate value\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\"Run the hyperparameter optimization.\n",
    "        \n",
    "         Returns\n",
    "         -------\n",
    "         dict\n",
    "             The best hyperparameters found by the optimization.\n",
    "        \"\"\"\n",
    "        study = optuna.create_study(direction='minimize',\n",
    "                                    pruner=MedianPruner())\n",
    "        study.optimize(self.objective, n_trials=self.num_trials)\n",
    "\n",
    "        best_hyperparams = study.best_trial.params\n",
    "        with open(f'gatv2_best_hyperparams_run_{RUN_NUMBER}.json', 'w') as f:\n",
    "            json.dump(best_hyperparams, f)\n",
    "        print(f\"Best hyperparameters are {best_hyperparams}.\")\n",
    "        print(\"Best hyperparameters saved.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b751b0",
   "metadata": {
    "papermill": {
     "duration": 0.090153,
     "end_time": "2024-03-15T07:43:37.774089",
     "exception": false,
     "start_time": "2024-03-15T07:43:37.683936",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Main Training Loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e459d7d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T07:43:37.848872Z",
     "iopub.status.busy": "2024-03-15T07:43:37.848593Z",
     "iopub.status.idle": "2024-03-15T07:43:38.024324Z",
     "shell.execute_reply": "2024-03-15T07:43:38.022607Z"
    },
    "papermill": {
     "duration": 0.245438,
     "end_time": "2024-03-15T07:43:38.028530",
     "exception": false,
     "start_time": "2024-03-15T07:43:37.783092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02873317",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T07:43:38.057288Z",
     "iopub.status.busy": "2024-03-15T07:43:38.056998Z",
     "iopub.status.idle": "2024-03-15T07:43:51.580245Z",
     "shell.execute_reply": "2024-03-15T07:43:51.578582Z"
    },
    "papermill": {
     "duration": 13.547221,
     "end_time": "2024-03-15T07:43:51.584257",
     "exception": false,
     "start_time": "2024-03-15T07:43:38.037036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the project...\n",
      "\n",
      "Starting data loading...\n",
      "Train: 47275, Validation: 11819, Test: 14774, \n",
      "Subset Train: 14182, Subset Val: 3545\n",
      "\n",
      "Completed data loading.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data and prepare for training\n",
    "reloaded_df = pd.read_csv(\"data_mvi/combined_df.csv\")\n",
    "graphs, labels_dict = dgl.load_graphs(\"data_mvi/graphs.bin\")\n",
    "labels = reloaded_df['binds_to_rna'].values\n",
    "\n",
    "# Split dataset train, test\n",
    "train_indices, test_indices, train_labels, test_labels = train_test_split(\n",
    "    range(len(reloaded_df)), labels, test_size=0.2, stratify=labels,\n",
    "    random_state=42)\n",
    "\n",
    "# Split dataset train, validation\n",
    "train_indices, val_indices, train_labels, val_labels = train_test_split(\n",
    "    train_indices, train_labels, test_size=0.2, stratify=train_labels,\n",
    "    random_state=42)\n",
    "\n",
    "# Placeholder for data loading. Replace this with your actual data loading\n",
    "# code.\n",
    "train_graphs = [graphs[i] for i in train_indices]\n",
    "test_graphs = [graphs[i] for i in test_indices]\n",
    "val_graphs = [graphs[i] for i in val_indices]\n",
    "\n",
    "subset_train_indices = np.random.choice(\n",
    "    len(train_graphs), size=int(len(train_graphs) * 0.3), replace=False)\n",
    "subset_train_graphs = [train_graphs[i] for i in subset_train_indices]\n",
    "subset_train_labels = train_labels[subset_train_indices]\n",
    "\n",
    "subset_val_indices = np.random.choice(\n",
    "    len(val_graphs), size=int(len(val_graphs) * 0.3), replace=False)\n",
    "subset_val_graphs = [val_graphs[i] for i in subset_val_indices]\n",
    "subset_val_labels = val_labels[subset_val_indices]\n",
    "\n",
    "# Combine train and validation graphs and labels for retraining\n",
    "combined_train_graphs = train_graphs + val_graphs\n",
    "combined_train_labels = np.concatenate((train_labels, val_labels))\n",
    "\n",
    "# annouce the start of the project\n",
    "print(\"Starting the project...\")\n",
    "print(\"\")\n",
    "\n",
    "# annouce the start of the data loading\n",
    "print(\"Starting data loading...\")\n",
    "print(\n",
    "    f'Train: {len(train_graphs)}, Validation: {len(val_graphs)}, '\n",
    "    f'Test: {len(test_graphs)}, \\nSubset Train: {len(subset_train_graphs)}, '\n",
    "    f'Subset Val: {len(subset_val_graphs)}'\n",
    ")\n",
    "print(\"\")\n",
    "print(\"Completed data loading.\")\n",
    "print(\"\")\n",
    "sys.stdout.flush()  # Force flushing of the buffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c2dc99",
   "metadata": {
    "papermill": {
     "duration": 0.008308,
     "end_time": "2024-03-15T07:43:51.601582",
     "exception": false,
     "start_time": "2024-03-15T07:43:51.593274",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Perform Hyperparameter Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8796031",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T07:43:51.739390Z",
     "iopub.status.busy": "2024-03-15T07:43:51.739089Z",
     "shell.execute_reply": "2024-03-15T08:19:20.574604Z"
    },
    "papermill": {
     "duration": 2126.784653,
     "end_time": "2024-03-15T08:19:18.449552",
     "exception": false,
     "start_time": "2024-03-15T07:43:51.664899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Hyperparameter Optimization on a subset of the data\n",
    "print(\"Starting hyperparameter optimization...\")\n",
    "sys.stdout.flush()\n",
    "print(\"\")\n",
    "\n",
    "# Specify the number of trials and epochs for hyperparameter optimization\n",
    "optimizer = HyperparameterOptimizer(\n",
    "    device,\n",
    "    subset_train_graphs,\n",
    "    subset_train_labels,\n",
    "    subset_val_graphs,\n",
    "    subset_val_labels,\n",
    "    num_trials=50,\n",
    "    num_epochs=30)\n",
    "optimizer.optimize()\n",
    "print(\"Completed hyperparameter optimization.\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59169a32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T22:37:57.517278Z",
     "iopub.status.busy": "2024-03-14T22:37:57.516906Z",
     "iopub.status.idle": "2024-03-14T22:37:58.074003Z",
     "shell.execute_reply": "2024-03-14T22:37:58.072395Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the best hyperparameters\n",
    "with open('gatv2_best_hyperparams_run_{RUN_NUMBER}.json', 'r') as f:\n",
    "    best_hyperparams = json.load(f)\n",
    "\n",
    "train_loader = GraphDataLoader(list(zip(train_graphs,\n",
    "                                        train_labels)),\n",
    "                                batch_size=best_hyperparams['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=collate,\n",
    "                                num_workers=8)\n",
    "val_loader = GraphDataLoader(list(zip(val_graphs,\n",
    "                                        val_labels)),\n",
    "                                batch_size=best_hyperparams['batch_size'],\n",
    "                                shuffle=False,\n",
    "                                collate_fn=collate,\n",
    "                                num_workers=8)\n",
    "test_loader = GraphDataLoader(list(zip(test_graphs,\n",
    "                                        test_labels)),\n",
    "                                batch_size=best_hyperparams['batch_size'],\n",
    "                                shuffle=False,\n",
    "                                collate_fn=collate,\n",
    "                                num_workers=8)\n",
    "combined_train_loader = GraphDataLoader(\n",
    "    list(\n",
    "        zip(\n",
    "            combined_train_graphs,\n",
    "            combined_train_labels)),\n",
    "    batch_size=best_hyperparams['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate,\n",
    "    num_workers=8)\n",
    "print(\"Data loaders created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ed9fc5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Retraining with best hyperparameters (on a larger train and val set)\n",
    "print(\"Retraining with best hyperparameters...\")\n",
    "model = GraphClsGATv2(\n",
    "    in_feats=74,  # Adjust this based on your dataset\n",
    "    hidden_dim=best_hyperparams['hidden_dim'],\n",
    "    num_heads=best_hyperparams['num_heads'],\n",
    "    feat_drop=best_hyperparams['feat_drop'],\n",
    "    attn_drop=best_hyperparams['attn_drop'],\n",
    "    negative_slope=best_hyperparams['negative_slope'],\n",
    "    num_cls=2,  # Adjust based on your dataset\n",
    ").to(device)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Reset model weights and biases parameters before retraining\n",
    "model.reset_parameters()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_hyperparams['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "early_stopping = EarlyStopping(patience=35, verbose=True)\n",
    "training_pipeline = TrainingPipeline(device)\n",
    "training_pipeline.train_and_evaluate(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    early_stopping,\n",
    "    500,\n",
    "    plot_curves=True)\n",
    "optimal_epoch = early_stopping.best_epoch\n",
    "\n",
    "# Before final training on the combined train and val dataset, reset the\n",
    "# model weights and biases again\n",
    "model.reset_parameters()\n",
    "print(\"Completed training.\")\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169fa67",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train Model with Best Hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff248f50",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Final training on the combined train and val dataset\n",
    "print(\"Final training on the combined train and val dataset...\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_hyperparams['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=20, verbose=True, delta=0, path='checkpoint.pt')\n",
    "training_pipeline.train_and_evaluate(\n",
    "    model,\n",
    "    combined_train_loader,\n",
    "    None,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    None,\n",
    "    optimal_epoch,\n",
    "    plot_curves=False)\n",
    "print(\"Completed training.\")\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15590e15",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluate the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5767c39",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation on the test set\n",
    "print(\"Evaluating on the test set...\")\n",
    "training_pipeline.evaluate_on_test(model, test_loader, criterion)\n",
    "print(\"Completed evaluation.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2148.294818,
   "end_time": "2024-03-15T08:19:21.397080",
   "environment_variables": {},
   "exception": null,
   "input_path": "gatv2conv_cls_run_1.ipynb",
   "output_path": "gatv2conv_cls_run_1.ipynb",
   "parameters": {},
   "start_time": "2024-03-15T07:43:33.102262",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
