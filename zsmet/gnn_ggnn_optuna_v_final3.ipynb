{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8705dfb0",
   "metadata": {
    "papermill": {
     "duration": 0.00662,
     "end_time": "2024-03-11T20:29:08.784582",
     "exception": false,
     "start_time": "2024-03-11T20:29:08.777962",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GGNN Optuna Optimization\n",
    "This project demonstrates the use of a Gated Graph Neural Network (GGNN) model for a graph classification task. It includes the setup for GPU utilization, data loading and preparation, model definition, and the use of Optuna for hyperparameter optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e04b413",
   "metadata": {
    "papermill": {
     "duration": 0.024879,
     "end_time": "2024-03-11T20:29:08.817553",
     "exception": false,
     "start_time": "2024-03-11T20:29:08.792674",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Check GPU Availability\n",
    "This section checks the availability of a GPU for PyTorch, ensuring that model training can leverage hardware acceleration if available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10132d76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T20:29:08.835788Z",
     "iopub.status.busy": "2024-03-11T20:29:08.835356Z",
     "iopub.status.idle": "2024-03-11T20:29:12.203209Z",
     "shell.execute_reply": "2024-03-11T20:29:12.200917Z"
    },
    "papermill": {
     "duration": 3.37916,
     "end_time": "2024-03-11T20:29:12.207263",
     "exception": false,
     "start_time": "2024-03-11T20:29:08.828103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.2\n",
      "Is CUDA Supported? True\n",
      "1 CUDA device(s) available.\n",
      "CUDA Device Name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from dgl.nn import GatedGraphConv, GlobalAttentionPooling\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Check CUDA availability\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"Is CUDA Supported?\", cuda_available)\n",
    "if cuda_available:\n",
    "    print(torch.cuda.device_count(), \"CUDA device(s) available.\")\n",
    "    print(\"CUDA Device Name:\", torch.cuda.get_device_name(0))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7cc8e",
   "metadata": {
    "papermill": {
     "duration": 0.026696,
     "end_time": "2024-03-11T20:29:12.239968",
     "exception": false,
     "start_time": "2024-03-11T20:29:12.213272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Model and Utilities\n",
    "This section defines the GGNN model, an early stopping utility to prevent overfitting, and a custom collate function for data loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc1c7c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T20:29:12.282273Z",
     "iopub.status.busy": "2024-03-11T20:29:12.281396Z",
     "iopub.status.idle": "2024-03-11T20:29:12.297310Z",
     "shell.execute_reply": "2024-03-11T20:29:12.295596Z"
    },
    "papermill": {
     "duration": 0.040196,
     "end_time": "2024-03-11T20:29:12.300893",
     "exception": false,
     "start_time": "2024-03-11T20:29:12.260697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphClsGGNN(nn.Module):\n",
    "    \"\"\"GGNN for graph classification.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            annotation_size,\n",
    "            out_feats,\n",
    "            n_steps,\n",
    "            n_etypes,\n",
    "            num_cls,\n",
    "            dropout_rate=0.5):\n",
    "        super(GraphClsGGNN, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.ggnn1 = GatedGraphConv(\n",
    "            annotation_size, out_feats, n_steps, n_etypes)\n",
    "        self.ggnn2 = GatedGraphConv(out_feats, out_feats, n_steps, n_etypes)\n",
    "        self.pooling = GlobalAttentionPooling(nn.Linear(out_feats, 1))\n",
    "        self.fc = nn.Linear(out_feats, num_cls)\n",
    "\n",
    "    def forward(self, graph, feat):\n",
    "        h = F.relu(self.ggnn1(graph, feat))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.ggnn2(graph, h))\n",
    "        hg = self.pooling(graph, h)\n",
    "        return self.fc(hg)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.children():\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "\n",
    "# Define EarlyStopping class\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation \n",
    "    loss doesn't improve after a given patience.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            patience=7,\n",
    "            verbose=True,\n",
    "            delta=0.001,\n",
    "            path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.best_epoch = 0\n",
    "\n",
    "    def __call__(self, val_loss, model, epoch):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.best_epoch = epoch\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    f'EarlyStopping counter: {self.counter} '\n",
    "                    f'out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "            self.best_epoch = epoch\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f'Validation loss decreased ({self.val_loss_min:.6f} '\n",
    "                f'--> {val_loss:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# Define the collate function for the DataLoader\n",
    "\n",
    "\n",
    "def collate(samples):\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return batched_graph, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00faf28d",
   "metadata": {
    "papermill": {
     "duration": 0.004502,
     "end_time": "2024-03-11T20:29:12.310223",
     "exception": false,
     "start_time": "2024-03-11T20:29:12.305721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training and Evaluation Pipeline\n",
    "Outlines the process for training the GGNN model, including the training loop, validation checks, and early stopping implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e12f0178",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T20:29:12.323997Z",
     "iopub.status.busy": "2024-03-11T20:29:12.323489Z",
     "iopub.status.idle": "2024-03-11T20:29:12.340565Z",
     "shell.execute_reply": "2024-03-11T20:29:12.338986Z"
    },
    "papermill": {
     "duration": 0.028741,
     "end_time": "2024-03-11T20:29:12.344118",
     "exception": false,
     "start_time": "2024-03-11T20:29:12.315377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainingPipeline:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "    def train_and_evaluate(\n",
    "            self,\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            early_stopping,\n",
    "            num_epochs,\n",
    "            plot_curves=False,\n",
    "            accumulation_steps=8):\n",
    "        train_losses, val_losses = [], []\n",
    "        scaler = GradScaler()  # Initialize the gradient scaler\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            optimizer.zero_grad()  # Initialize gradients to zero\n",
    "\n",
    "            for batch_idx, (batched_graph, labels) in enumerate(train_loader):\n",
    "                batched_graph, labels = batched_graph.to(\n",
    "                    self.device), labels.to(self.device)\n",
    "\n",
    "                with autocast():  # Enable automatic mixed precision\n",
    "                    logits = model(\n",
    "                        batched_graph, batched_graph.ndata['h'].float())\n",
    "                    loss = criterion(logits, labels) / \\\n",
    "                        accumulation_steps  # Scale loss\n",
    "\n",
    "                # Scale the loss and call backward to propagate gradients\n",
    "                scaler.scale(loss).backward()\n",
    "                # Correct scaling for logging purposes\n",
    "                train_loss += loss.item() * accumulation_steps\n",
    "\n",
    "                if (batch_idx + 1) % accumulation_steps == 0 or \\\n",
    "                    batch_idx == len(train_loader) - 1:\n",
    "                    # Perform optimizer step using scaled gradients\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()  # Update the scaler for the next iteration\n",
    "                    optimizer.zero_grad()  # Initialize gradients to zero\n",
    "\n",
    "            train_loss /= len(train_loader)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            total = 0\n",
    "            if val_loader is not None:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_correct = 0\n",
    "                    total = 0\n",
    "                    for batched_graph, labels in val_loader:\n",
    "                        batched_graph, labels = batched_graph.to(\n",
    "                            self.device), labels.to(self.device)\n",
    "                        with autocast():  # Enable automatic mixed precision\n",
    "                            logits = model(\n",
    "                                batched_graph, batched_graph.ndata['h'].float()\n",
    "                                )\n",
    "                            loss = criterion(logits, labels)\n",
    "                        val_loss += loss.item()\n",
    "                        _, predicted = torch.max(logits.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "                    val_loss /= len(val_loader)\n",
    "                    val_losses.append(val_loss)\n",
    "                    val_accuracy = val_correct / total\n",
    "\n",
    "                    if early_stopping:\n",
    "                        early_stopping(val_loss, model, epoch + 1)\n",
    "                        if early_stopping.early_stop:\n",
    "                            print(\n",
    "                                f\"Early stopping triggered\"\n",
    "                                f\"at epoch {epoch + 1}\")\n",
    "                            break\n",
    "\n",
    "                if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                    print(\n",
    "                        f'Epoch {epoch + 1}/{num_epochs} - '\n",
    "                        f'Train Loss: {train_loss:.4f}, '\n",
    "                        f'Val Loss: {val_loss:.4f} '\n",
    "                        f'| Val accuracy: {100 * val_accuracy:.2f}%')\n",
    "\n",
    "        if plot_curves and val_loader is not None:\n",
    "            self.plot_loss_curves(train_losses, val_losses)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_loss_curves(train_losses, val_losses):\n",
    "        sns.set(style=\"whitegrid\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "        plt.plot(epochs, train_losses, label='Training Loss')\n",
    "        plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig('loss_curves.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate_on_test(self, model, test_loader, criterion):\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_accuracy = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batched_graph, labels in test_loader:\n",
    "                batched_graph, labels = batched_graph.to(\n",
    "                    self.device), labels.to(self.device)\n",
    "                logits = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "                loss = criterion(logits, labels)\n",
    "                test_loss += loss.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                test_accuracy += torch.sum(preds == labels).item()\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_accuracy /= len(test_loader.dataset)\n",
    "        print(\"Test Loss:\", test_loss)\n",
    "        print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a003e1e0",
   "metadata": {
    "papermill": {
     "duration": 0.004432,
     "end_time": "2024-03-11T20:29:12.353170",
     "exception": false,
     "start_time": "2024-03-11T20:29:12.348738",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hyperparameter Optimization with Optuna\n",
    "Describes the setup for hyperparameter optimization using Optuna, including defining the search space and optimizing the model parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b94e0db3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T20:29:12.364435Z",
     "iopub.status.busy": "2024-03-11T20:29:12.364142Z",
     "iopub.status.idle": "2024-03-11T20:29:12.377768Z",
     "shell.execute_reply": "2024-03-11T20:29:12.376437Z"
    },
    "papermill": {
     "duration": 0.023354,
     "end_time": "2024-03-11T20:29:12.381134",
     "exception": false,
     "start_time": "2024-03-11T20:29:12.357780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HyperparameterOptimizer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            device,\n",
    "            subset_train_graphs,\n",
    "            subset_train_labels,\n",
    "            subset_val_graphs,\n",
    "            subset_val_labels,\n",
    "            num_trials,\n",
    "            num_epochs):\n",
    "        self.device = device\n",
    "        self.subset_train_graphs = subset_train_graphs\n",
    "        self.subset_train_labels = subset_train_labels\n",
    "        self.subset_val_graphs = subset_val_graphs\n",
    "        self.subset_val_labels = subset_val_labels\n",
    "        self.num_trials = num_trials\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def objective(self, trial):\n",
    "        # Suggest hyperparameters\n",
    "        n_steps = trial.suggest_int('n_steps', 1, 40)\n",
    "        out_feats = trial.suggest_int('out_feats', 74, 512)\n",
    "        lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "        batch_size = trial.suggest_categorical(\n",
    "            'batch_size', [16, 32, 64, 128, 256, 512])\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "\n",
    "        # Create the model, optimizer, and loaders\n",
    "        model = GraphClsGGNN(\n",
    "            annotation_size=74,\n",
    "            out_feats=out_feats,\n",
    "            n_steps=n_steps,\n",
    "            n_etypes=1,\n",
    "            num_cls=2,\n",
    "            dropout_rate=dropout_rate).to(\n",
    "            self.device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        train_loader = GraphDataLoader(\n",
    "            list(\n",
    "                zip(\n",
    "                    self.subset_train_graphs,\n",
    "                    self.subset_train_labels)),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate,\n",
    "            num_workers=3)\n",
    "        val_loader = GraphDataLoader(\n",
    "            list(\n",
    "                zip(\n",
    "                    self.subset_val_graphs,\n",
    "                    self.subset_val_labels)),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=collate,\n",
    "            num_workers=3)\n",
    "\n",
    "        # Training loop with pruning\n",
    "        model.train()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for batched_graph, labels in train_loader:\n",
    "                batched_graph, labels = batched_graph.to(\n",
    "                    self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "                loss = criterion(logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Validation phase and report for pruning\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batched_graph, labels in val_loader:\n",
    "                    batched_graph, labels = batched_graph.to(\n",
    "                        self.device), labels.to(self.device)\n",
    "                    logits = model(\n",
    "                        batched_graph, batched_graph.ndata['h'].float())\n",
    "                    loss = criterion(logits, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            # Report intermediate value to the pruner\n",
    "            trial.report(val_loss, epoch)\n",
    "\n",
    "            if trial.should_prune(): # Handle pruning based on the intermediate\n",
    "                                                                        # value\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        return val_loss  # Return the validation loss\n",
    "\n",
    "    def optimize(self):\n",
    "        # Use MedianPruner in the study to activate pruning\n",
    "        study = optuna.create_study(\n",
    "            direction='minimize', pruner=MedianPruner())\n",
    "        study.optimize(self.objective, n_trials=self.num_trials)\n",
    "\n",
    "        # Save and print the best hyperparameters\n",
    "        best_hyperparams = study.best_trial.params\n",
    "        with open('best_hyperparameters.json', 'w') as f:\n",
    "            json.dump(best_hyperparams, f)\n",
    "        print(f\"Best hyperparameters are {optimizer.best_hyperparams}.\")\n",
    "        print(\"Best hyperparameters saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9aea08",
   "metadata": {
    "papermill": {
     "duration": 0.004579,
     "end_time": "2024-03-11T20:29:12.390600",
     "exception": false,
     "start_time": "2024-03-11T20:29:12.386021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hyperparameter Optimization Execution, Retraining and Evaluation on test set\n",
    "Initiates the hyperparameter optimization process, leveraging the previously defined model, data loaders, and Optuna setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1476224d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T20:29:12.401576Z",
     "iopub.status.busy": "2024-03-11T20:29:12.401298Z",
     "iopub.status.idle": "2024-03-11T20:37:07.857462Z",
     "shell.execute_reply": "2024-03-11T20:37:07.855451Z"
    },
    "papermill": {
     "duration": 474.054694,
     "end_time": "2024-03-11T20:37:06.450239",
     "exception": false,
     "start_time": "2024-03-11T20:29:12.395545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load data and prepare for training\n",
    "    reloaded_df = pd.read_csv(\"data_mvi/combined_df.csv\")\n",
    "    graphs, labels_dict = dgl.load_graphs(\"data_mvi/graphs.bin\")\n",
    "    labels = reloaded_df['binds_to_rna'].values\n",
    "\n",
    "    # Split dataset train, test\n",
    "    train_indices, test_indices, train_labels, test_labels = train_test_split(\n",
    "        range(len(reloaded_df)), labels, test_size=0.2, stratify=labels, \n",
    "        random_state=42)\n",
    "\n",
    "    # Split dataset train, validation\n",
    "    train_indices, val_indices, train_labels, val_labels = train_test_split(\n",
    "        train_indices, train_labels, test_size=0.2, stratify=train_labels, \n",
    "        random_state=42)\n",
    "\n",
    "    # Placeholder for data loading. Replace this with your actual data loading\n",
    "    # code.\n",
    "    train_graphs = [graphs[i] for i in train_indices]\n",
    "    test_graphs = [graphs[i] for i in test_indices]\n",
    "    val_graphs = [graphs[i] for i in val_indices]\n",
    "\n",
    "    subset_train_indices = np.random.choice(\n",
    "        len(train_graphs), size=int(len(train_graphs) * 0.3), replace=False)\n",
    "    subset_train_graphs = [train_graphs[i] for i in subset_train_indices]\n",
    "    subset_train_labels = train_labels[subset_train_indices]\n",
    "\n",
    "    subset_val_indices = np.random.choice(\n",
    "        len(val_graphs), size=int(len(val_graphs) * 0.3), replace=False)\n",
    "    subset_val_graphs = [val_graphs[i] for i in subset_val_indices]\n",
    "    subset_val_labels = val_labels[subset_val_indices]\n",
    "\n",
    "    # Combine train and validation graphs and labels for retraining\n",
    "    combined_train_graphs = train_graphs + val_graphs\n",
    "    combined_train_labels = np.concatenate((train_labels, val_labels))\n",
    "\n",
    "    print(\n",
    "        f'Train: {len(train_graphs)}, Validation: {len(val_graphs)}, '\n",
    "        f'Test: {len(test_graphs)}, Subset Train: {len(subset_train_graphs)}, '\n",
    "        f'Subset Val: {len(subset_val_graphs)}'\n",
    "    )\n",
    "    print(\"\")\n",
    "    print(\"Completed data loading.\")\n",
    "    print(\"\")\n",
    "    sys.stdout.flush()  # Force flushing of the buffer\n",
    "\n",
    "    # 1. Hyperparameter Optimization on a subset of the data\n",
    "    print(\"Starting hyperparameter optimization...\")\n",
    "    sys.stdout.flush()\n",
    "    print(\"\")\n",
    "\n",
    "    # Specify the number of trials and epochs for hyperparameter optimization\n",
    "    optimizer = HyperparameterOptimizer(\n",
    "        device,\n",
    "        subset_train_graphs,\n",
    "        subset_train_labels,\n",
    "        subset_val_graphs,\n",
    "        subset_val_labels,\n",
    "        num_trials=16,\n",
    "        num_epochs=20)\n",
    "    optimizer.optimize()\n",
    "    print(\"Completed hyperparameter optimization.\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    # Load the best hyperparameters\n",
    "    with open('best_hyperparameters.json', 'r') as f:\n",
    "        best_hyperparams = json.load(f)\n",
    "\n",
    "    # Correcting the use of best_hyperparams by\n",
    "    train_loader = GraphDataLoader(list(zip(train_graphs,\n",
    "                                            train_labels)),\n",
    "                                   batch_size=best_hyperparams['batch_size'],\n",
    "                                   shuffle=True,\n",
    "                                   collate_fn=collate,\n",
    "                                   num_workers=8)\n",
    "    val_loader = GraphDataLoader(list(zip(val_graphs,\n",
    "                                          val_labels)),\n",
    "                                 batch_size=best_hyperparams['batch_size'],\n",
    "                                 shuffle=False,\n",
    "                                 collate_fn=collate,\n",
    "                                 num_workers=8)\n",
    "    test_loader = GraphDataLoader(list(zip(test_graphs,\n",
    "                                           test_labels)),\n",
    "                                  batch_size=best_hyperparams['batch_size'],\n",
    "                                  shuffle=False,\n",
    "                                  collate_fn=collate,\n",
    "                                  num_workers=8)\n",
    "    combined_train_loader = GraphDataLoader(\n",
    "        list(\n",
    "            zip(\n",
    "                combined_train_graphs,\n",
    "                combined_train_labels)),\n",
    "        batch_size=best_hyperparams['batch_size'],\n",
    "        shuffle=True,\n",
    "        collate_fn=collate,\n",
    "        num_workers=8)\n",
    "\n",
    "    # 2. Retraining with best hyperparameters (on a larger train and val set)\n",
    "    print(\"Retraining with best hyperparameters...\")\n",
    "    model = GraphClsGGNN(\n",
    "        annotation_size=74,\n",
    "        out_feats=best_hyperparams['out_feats'],\n",
    "        n_steps=best_hyperparams['n_steps'],\n",
    "        n_etypes=1,\n",
    "        num_cls=2,\n",
    "        dropout_rate=best_hyperparams['dropout_rate']).to(device)\n",
    "    print(\"\")\n",
    "\n",
    "    # Reset model parameters before retraining\n",
    "    model.reset_parameters()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_hyperparams['lr'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=35, verbose=True)\n",
    "    training_pipeline = TrainingPipeline(device)\n",
    "    training_pipeline.train_and_evaluate(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        early_stopping,\n",
    "        500,\n",
    "        plot_curves=True)\n",
    "    optimal_epoch = early_stopping.best_epoch\n",
    "\n",
    "    # Before final training on the combined train and val dataset, reset the\n",
    "    # model again\n",
    "    model.reset_parameters()\n",
    "    print(\"Completed training.\")\n",
    "    print(\"\")\n",
    "\n",
    "    # 3. Final training on the combined train and val dataset\n",
    "    print(\"Final training on the combined train and val dataset...\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_hyperparams['lr'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=20, verbose=True, delta=0, path='final_model.pt')\n",
    "    training_pipeline.train_and_evaluate(\n",
    "        model,\n",
    "        combined_train_loader,\n",
    "        None,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        None,\n",
    "        optimal_epoch,\n",
    "        plot_curves=False)\n",
    "    print(\"Completed training.\")\n",
    "    print(\"\")\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    print(\"Evaluating on the test set...\")\n",
    "    training_pipeline.evaluate_on_test(model, test_loader, criterion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 482.040554,
   "end_time": "2024-03-11T20:37:09.477545",
   "environment_variables": {},
   "exception": null,
   "input_path": "gnn_ggnn_optuna_v_final3.ipynb",
   "output_path": "gnn_ggnn_optuna_v_final3.ipynb",
   "parameters": {},
   "start_time": "2024-03-11T20:29:07.436991",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
