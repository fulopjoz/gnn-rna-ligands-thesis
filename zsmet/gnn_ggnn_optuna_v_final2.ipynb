{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8705dfb0",
   "metadata": {
    "papermill": {
     "duration": 0.006413,
     "end_time": "2024-03-11T17:28:56.375145",
     "exception": false,
     "start_time": "2024-03-11T17:28:56.368732",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GGNN Optuna Optimization\n",
    "This project demonstrates the use of a Gated Graph Neural Network (GGNN) model for a graph classification task. It includes the setup for GPU utilization, data loading and preparation, model definition, and the use of Optuna for hyperparameter optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e04b413",
   "metadata": {
    "papermill": {
     "duration": 0.004386,
     "end_time": "2024-03-11T17:28:56.387624",
     "exception": false,
     "start_time": "2024-03-11T17:28:56.383238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Check GPU Availability\n",
    "This section checks the availability of a GPU for PyTorch, ensuring that model training can leverage hardware acceleration if available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10132d76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T17:28:56.399257Z",
     "iopub.status.busy": "2024-03-11T17:28:56.398644Z",
     "iopub.status.idle": "2024-03-11T17:28:59.516095Z",
     "shell.execute_reply": "2024-03-11T17:28:59.514167Z"
    },
    "papermill": {
     "duration": 3.128052,
     "end_time": "2024-03-11T17:28:59.520076",
     "exception": false,
     "start_time": "2024-03-11T17:28:56.392024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.2\n",
      "Is CUDA Supported? True\n",
      "1 CUDA device(s) available.\n",
      "CUDA Device Name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dgl\n",
    "from dgl.nn import GatedGraphConv, GlobalAttentionPooling\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "# Check CUDA availability\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"Is CUDA Supported?\", cuda_available)\n",
    "if cuda_available:\n",
    "    print(torch.cuda.device_count(), \"CUDA device(s) available.\")\n",
    "    print(\"CUDA Device Name:\", torch.cuda.get_device_name(0))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7cc8e",
   "metadata": {
    "papermill": {
     "duration": 0.004691,
     "end_time": "2024-03-11T17:28:59.530431",
     "exception": false,
     "start_time": "2024-03-11T17:28:59.525740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Model and Utilities\n",
    "This section defines the GGNN model, an early stopping utility to prevent overfitting, and a custom collate function for data loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc1c7c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T17:28:59.542050Z",
     "iopub.status.busy": "2024-03-11T17:28:59.541389Z",
     "iopub.status.idle": "2024-03-11T17:28:59.555956Z",
     "shell.execute_reply": "2024-03-11T17:28:59.554349Z"
    },
    "papermill": {
     "duration": 0.024376,
     "end_time": "2024-03-11T17:28:59.559436",
     "exception": false,
     "start_time": "2024-03-11T17:28:59.535060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class GraphClsGGNN(nn.Module):\n",
    "    \"\"\"GGNN for graph classification.\"\"\"\n",
    "    def __init__(self, annotation_size, out_feats, n_steps, n_etypes, num_cls, dropout_rate=0.5):\n",
    "        super(GraphClsGGNN, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.ggnn1 = GatedGraphConv(annotation_size, out_feats, n_steps, n_etypes)\n",
    "        self.ggnn2 = GatedGraphConv(out_feats, out_feats, n_steps, n_etypes)\n",
    "        self.pooling = GlobalAttentionPooling(nn.Linear(out_feats, 1))\n",
    "        self.fc = nn.Linear(out_feats, num_cls)\n",
    "\n",
    "    def forward(self, graph, feat):\n",
    "        h = F.relu(self.ggnn1(graph, feat))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.ggnn2(graph, h))\n",
    "        hg = self.pooling(graph, h)\n",
    "        return self.fc(hg)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        for layer in self.children():\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                print(f\"Resetting parameters of layer = {layer}\")\n",
    "                layer.reset_parameters()\n",
    "\n",
    "# Define EarlyStopping class\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=True, delta=0.001, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# Define the collate function for the DataLoader\n",
    "def collate(samples):\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return batched_graph, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00faf28d",
   "metadata": {
    "papermill": {
     "duration": 0.004565,
     "end_time": "2024-03-11T17:28:59.568486",
     "exception": false,
     "start_time": "2024-03-11T17:28:59.563921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training and Evaluation Pipeline\n",
    "Outlines the process for training the GGNN model, including the training loop, validation checks, and early stopping implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e12f0178",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T17:28:59.579284Z",
     "iopub.status.busy": "2024-03-11T17:28:59.578982Z",
     "iopub.status.idle": "2024-03-11T17:28:59.595079Z",
     "shell.execute_reply": "2024-03-11T17:28:59.593475Z"
    },
    "papermill": {
     "duration": 0.025328,
     "end_time": "2024-03-11T17:28:59.598397",
     "exception": false,
     "start_time": "2024-03-11T17:28:59.573069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TrainingPipeline class encapsulating the training and evaluation process\n",
    "class TrainingPipeline:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "def train_and_evaluate(self, model, train_loader, val_loader, optimizer, criterion, early_stopping, num_epochs, plot_curves=False, accumulation_steps=4):\n",
    "    train_losses, val_losses = [], []\n",
    "    scaler = GradScaler()  # Initialize the gradient scaler\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        optimizer.zero_grad()  # Initialize gradients to zero\n",
    "\n",
    "        for batch_idx, (batched_graph, labels) in enumerate(train_loader):\n",
    "            batched_graph, labels = batched_graph.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            with autocast():  # Enable automatic mixed precision\n",
    "                logits = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "                loss = criterion(logits, labels) / accumulation_steps  # Scale loss\n",
    "\n",
    "            scaler.scale(loss).backward()  # Scale the loss and call backward to propagate gradients\n",
    "            train_loss += loss.item() * accumulation_steps  # Correct scaling for logging purposes\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or batch_idx == len(train_loader) - 1:\n",
    "                scaler.step(optimizer)  # Perform optimizer step using scaled gradients\n",
    "                scaler.update()  # Update the scaler for the next iteration\n",
    "                optimizer.zero_grad()  # Reset gradients for the next accumulation\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        val_loss = 0.0\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batched_graph, labels in val_loader:\n",
    "                    batched_graph, labels = batched_graph.to(self.device), labels.to(self.device)\n",
    "                    with autocast():  # Use autocast for the validation step\n",
    "                        logits = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "                        loss = criterion(logits, labels)\n",
    "                    val_loss += loss.item()\n",
    "            val_loss /= len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            if early_stopping:\n",
    "                early_stopping(val_loss, model)\n",
    "                if early_stopping.early_stop:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                    break\n",
    "\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    if plot_curves and val_loader is not None:\n",
    "        self.plot_loss_curves(train_losses, val_losses)\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "\n",
    "@staticmethod\n",
    "def plot_loss_curves(train_losses, val_losses):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.plot(epochs, train_losses, label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss Curves')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_on_test(self, model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batched_graph, labels in test_loader:\n",
    "            batched_graph, labels = batched_graph.to(self.device), labels.to(self.device)\n",
    "            logits = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "            loss = criterion(logits, labels)\n",
    "            test_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            test_accuracy += torch.sum(preds == labels).item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy /= len(test_loader.dataset)\n",
    "    print(\"Test Loss:\", test_loss)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a003e1e0",
   "metadata": {
    "papermill": {
     "duration": 0.004496,
     "end_time": "2024-03-11T17:28:59.607598",
     "exception": false,
     "start_time": "2024-03-11T17:28:59.603102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hyperparameter Optimization with Optuna\n",
    "Describes the setup for hyperparameter optimization using Optuna, including defining the search space and optimizing the model parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b94e0db3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T17:28:59.618755Z",
     "iopub.status.busy": "2024-03-11T17:28:59.618448Z",
     "iopub.status.idle": "2024-03-11T17:28:59.637368Z",
     "shell.execute_reply": "2024-03-11T17:28:59.635397Z"
    },
    "papermill": {
     "duration": 0.028125,
     "end_time": "2024-03-11T17:28:59.640469",
     "exception": false,
     "start_time": "2024-03-11T17:28:59.612344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HyperparameterOptimizer:\n",
    "    def __init__(self, device, subset_train_graphs, subset_train_labels, subset_val_graphs, subset_val_labels, num_trials, num_epochs):\n",
    "        self.device = device\n",
    "        self.subset_train_graphs = subset_train_graphs\n",
    "        self.subset_train_labels = subset_train_labels\n",
    "        self.subset_val_graphs = subset_val_graphs\n",
    "        self.subset_val_labels = subset_val_labels\n",
    "        self.num_trials = num_trials\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def objective(self, trial):\n",
    "        n_steps = trial.suggest_int('n_steps', 1, 40)\n",
    "        out_feats = trial.suggest_int('out_feats', 74, 512)\n",
    "        lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128, 256, 512])\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "            \n",
    "        model = GraphClsGGNN(annotation_size=74, out_feats=out_feats, n_steps=n_steps, n_etypes=1, num_cls=2, dropout_rate=dropout_rate).to(self.device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        train_loader = GraphDataLoader(list(zip(self.subset_train_graphs, self.subset_train_labels)), batch_size=batch_size, shuffle=True, collate_fn=collate, num_workers=4)\n",
    "        val_loader = GraphDataLoader(list(zip(self.subset_val_graphs, self.subset_val_labels)), batch_size=batch_size, shuffle=False, collate_fn=collate, num_workers=4)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for batched_graph, labels in train_loader:\n",
    "                batched_graph, labels = batched_graph.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "                loss = criterion(logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batched_graph, labels in val_loader:\n",
    "                batched_graph, labels = batched_graph.to(self.device), labels.to(self.device)\n",
    "                logits = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "                loss = criterion(logits, labels)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        average_val_loss = sum(val_losses) / len(val_losses)\n",
    "        return -average_val_loss\n",
    "\n",
    "    \n",
    "    \n",
    "    def optimize(self):\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(self.objective, n_trials=self.num_trials)\n",
    "        best_hyperparams = study.best_trial.params\n",
    "        with open('best_hyperparameters.json', 'w') as f:\n",
    "            json.dump(best_hyperparams, f)\n",
    "        print(\"Best hyperparameters saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9aea08",
   "metadata": {
    "papermill": {
     "duration": 0.003376,
     "end_time": "2024-03-11T17:28:59.648063",
     "exception": false,
     "start_time": "2024-03-11T17:28:59.644687",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hyperparameter Optimization Execution\n",
    "Initiates the hyperparameter optimization process, leveraging the previously defined model, data loaders, and Optuna setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1476224d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T17:28:59.656488Z",
     "iopub.status.busy": "2024-03-11T17:28:59.656231Z",
     "iopub.status.idle": "2024-03-11T19:18:00.161546Z",
     "shell.execute_reply": "2024-03-11T19:18:00.159564Z"
    },
    "papermill": {
     "duration": 6538.940458,
     "end_time": "2024-03-11T19:17:58.591968",
     "exception": false,
     "start_time": "2024-03-11T17:28:59.651510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-11 17:29:11,477] A new study created in memory with name: no-name-0a527522-1e96-4112-bc87-dab6c8e231da\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 47275, Validation: 11819, Test: 14774\n",
      "\n",
      "Starting hyperparameter optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-11 17:55:03,255] Trial 0 finished with value: -0.4938222929701075 and parameters: {'n_steps': 39, 'out_feats': 262, 'lr': 6.22483056273888e-05, 'batch_size': 32, 'dropout_rate': 0.37913485137697694}. Best is trial 0 with value: -0.4938222929701075.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-11 17:57:29,728] Trial 1 finished with value: -0.5164363852569035 and parameters: {'n_steps': 26, 'out_feats': 84, 'lr': 5.8635265964719503e-05, 'batch_size': 512, 'dropout_rate': 0.17833163712280603}. Best is trial 0 with value: -0.4938222929701075.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-11 17:59:14,480] Trial 2 finished with value: -0.38883412735802786 and parameters: {'n_steps': 4, 'out_feats': 245, 'lr': 0.0004428931009186564, 'batch_size': 512, 'dropout_rate': 0.25606394646413966}. Best is trial 2 with value: -0.38883412735802786.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-11 18:23:44,835] Trial 3 finished with value: -0.628852231161935 and parameters: {'n_steps': 24, 'out_feats': 396, 'lr': 0.00540635730267523, 'batch_size': 64, 'dropout_rate': 0.1930707329546718}. Best is trial 2 with value: -0.38883412735802786.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-11 18:34:05,209] Trial 4 finished with value: -0.7029176077864192 and parameters: {'n_steps': 11, 'out_feats': 121, 'lr': 0.004104937601498425, 'batch_size': 16, 'dropout_rate': 0.12390502352967875}. Best is trial 2 with value: -0.38883412735802786.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/xfulop/miniconda3/envs/gnn/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/xfulop/miniconda3/envs/gnn/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/xfulop/miniconda3/envs/gnn/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/xfulop/miniconda3/envs/gnn/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xfulop/miniconda3/envs/gnn/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/xfulop/miniconda3/envs/gnn/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/xfulop/miniconda3/envs/gnn/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/xfulop/miniconda3/envs/gnn/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-03-11 19:17:58,844] Trial 5 failed with parameters: {'n_steps': 37, 'out_feats': 501, 'lr': 0.0001610463802433714, 'batch_size': 128, 'dropout_rate': 0.18486609368210055} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xfulop/miniconda3/envs/gnn/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_26185/3110813015.py\", line 28, in objective\n",
      "    batched_graph, labels = batched_graph.to(self.device), labels.to(self.device)\n",
      "  File \"/home/xfulop/miniconda3/envs/gnn/lib/python3.8/site-packages/dgl/heterograph.py\", line 5730, in to\n",
      "    new_bnn = {\n",
      "  File \"/home/xfulop/miniconda3/envs/gnn/lib/python3.8/site-packages/dgl/heterograph.py\", line 5731, in <dictcomp>\n",
      "    k: F.copy_to(num, device, **kwargs)\n",
      "  File \"/home/xfulop/miniconda3/envs/gnn/lib/python3.8/site-packages/dgl/backend/pytorch/tensor.py\", line 143, in copy_to\n",
      "    return input.cuda(**kwargs)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-03-11 19:17:58,921] Trial 5 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting hyperparameter optimization...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m HyperparameterOptimizer(device, subset_train_graphs, subset_train_labels, subset_val_graphs, subset_val_labels, num_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Load the best hyperparameters\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 52\u001b[0m, in \u001b[0;36mHyperparameterOptimizer.optimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     51\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     best_hyperparams \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_hyperparameters.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/miniconda3/envs/gnn/lib/python3.8/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gnn/lib/python3.8/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gnn/lib/python3.8/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/envs/gnn/lib/python3.8/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/envs/gnn/lib/python3.8/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m, in \u001b[0;36mHyperparameterOptimizer.objective\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batched_graph, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 28\u001b[0m         batched_graph, labels \u001b[38;5;241m=\u001b[39m \u001b[43mbatched_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     29\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     30\u001b[0m         logits \u001b[38;5;241m=\u001b[39m model(batched_graph, batched_graph\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[0;32m~/miniconda3/envs/gnn/lib/python3.8/site-packages/dgl/heterograph.py:5730\u001b[0m, in \u001b[0;36mDGLGraph.to\u001b[0;34m(self, device, **kwargs)\u001b[0m\n\u001b[1;32m   5728\u001b[0m \u001b[38;5;66;03m# 2. Copy misc info\u001b[39;00m\n\u001b[1;32m   5729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_num_nodes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 5730\u001b[0m     new_bnn \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   5731\u001b[0m         k: F\u001b[38;5;241m.\u001b[39mcopy_to(num, device, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   5732\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_num_nodes\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   5733\u001b[0m     }\n\u001b[1;32m   5734\u001b[0m     ret\u001b[38;5;241m.\u001b[39m_batch_num_nodes \u001b[38;5;241m=\u001b[39m new_bnn\n\u001b[1;32m   5735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_num_edges \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gnn/lib/python3.8/site-packages/dgl/heterograph.py:5731\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   5728\u001b[0m \u001b[38;5;66;03m# 2. Copy misc info\u001b[39;00m\n\u001b[1;32m   5729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_num_nodes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5730\u001b[0m     new_bnn \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m-> 5731\u001b[0m         k: \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5732\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_num_nodes\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   5733\u001b[0m     }\n\u001b[1;32m   5734\u001b[0m     ret\u001b[38;5;241m.\u001b[39m_batch_num_nodes \u001b[38;5;241m=\u001b[39m new_bnn\n\u001b[1;32m   5735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_num_edges \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gnn/lib/python3.8/site-packages/dgl/backend/pytorch/tensor.py:143\u001b[0m, in \u001b[0;36mcopy_to\u001b[0;34m(input, ctx, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m         th\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(ctx\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid context\u001b[39m\u001b[38;5;124m\"\u001b[39m, ctx)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load data and prepare for training\n",
    "    reloaded_df = pd.read_csv(\"data_mvi/combined_df.csv\")\n",
    "    graphs, labels_dict = dgl.load_graphs(\"data_mvi/graphs.bin\")\n",
    "    labels = reloaded_df['binds_to_rna'].values\n",
    "\n",
    "    # Split dataset train, test\n",
    "    train_indices, test_indices, train_labels, test_labels = train_test_split(\n",
    "        range(len(reloaded_df)), labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "\n",
    "    # Split dataset train, validation\n",
    "    train_indices, val_indices, train_labels, val_labels = train_test_split(\n",
    "        train_indices, train_labels, test_size=0.2, stratify=train_labels, random_state=42)\n",
    "    \n",
    "    # Placeholder for data loading. Replace this with your actual data loading code.\n",
    "    train_graphs = [graphs[i] for i in train_indices]\n",
    "    test_graphs = [graphs[i] for i in test_indices]\n",
    "    val_graphs = [graphs[i] for i in val_indices]\n",
    "    \n",
    "    subset_train_indices = np.random.choice(len(train_graphs), size=int(len(train_graphs) * 0.3), replace=False)\n",
    "    subset_train_graphs = [train_graphs[i] for i in subset_train_indices]\n",
    "    subset_train_labels = train_labels[subset_train_indices]\n",
    "\n",
    "    subset_val_indices = np.random.choice(len(val_graphs), size=int(len(val_graphs) * 0.3), replace=False)\n",
    "    subset_val_graphs = [val_graphs[i] for i in subset_val_indices]\n",
    "    subset_val_labels = val_labels[subset_val_indices]\n",
    "    \n",
    "    # Combine train and validation graphs and labels for retraining\n",
    "    combined_train_graphs = train_graphs + val_graphs\n",
    "    combined_train_labels = np.concatenate((train_labels, val_labels))\n",
    "    \n",
    "    # Define a fixed batch size for subset data loaders used in hyperparameter optimization\n",
    "    fixed_batch_size = 32  # This is a placeholder value\n",
    "\n",
    "    # Data Loaders for the subset used in hyperparameter optimization\n",
    "    subset_train_loader = GraphDataLoader(list(zip(subset_train_graphs, subset_train_labels)), batch_size=fixed_batch_size, shuffle=True, collate_fn=collate, num_workers=4)\n",
    "    subset_val_loader = GraphDataLoader(list(zip(subset_val_graphs, subset_val_labels)), batch_size=fixed_batch_size, shuffle=False, collate_fn=collate, num_workers=4)\n",
    "\n",
    "    print(f'Train: {len(train_graphs)}, Validation: {len(val_graphs)}, Test: {len(test_graphs)}')\n",
    "    print(\"\")\n",
    "\n",
    "    # Specify the number of trials and epochs for hyperparameter optimization\n",
    "    num_trials = 2  # 10\n",
    "    num_epochs_optimization = 3  # 20\n",
    "    \n",
    "    # 1. Hyperparameter Optimization on a subset of the data\n",
    "    print(\"Starting hyperparameter optimization...\")\n",
    "    optimizer = HyperparameterOptimizer(device, subset_train_graphs, subset_train_labels, subset_val_graphs, subset_val_labels, num_trials=10, num_epochs=20)\n",
    "    optimizer.optimize()\n",
    "    \n",
    "    \n",
    "    print(\"\")\n",
    "    # Load the best hyperparameters\n",
    "    with open('best_hyperparameters.json', 'r') as f:\n",
    "        best_hyperparams = json.load(f)\n",
    "\n",
    "    # Correcting the use of best_hyperparams by defining data loaders here\n",
    "    train_loader = GraphDataLoader(list(zip(train_graphs, train_labels)), batch_size=best_hyperparams['batch_size'], shuffle=True, collate_fn=collate, num_workers=4)\n",
    "    val_loader = GraphDataLoader(list(zip(val_graphs, val_labels)), batch_size=best_hyperparams['batch_size'], shuffle=False, collate_fn=collate, num_workers=4)\n",
    "    test_loader = GraphDataLoader(list(zip(test_graphs, test_labels)), batch_size=best_hyperparams['batch_size'], shuffle=False, collate_fn=collate, num_workers=4)\n",
    "    combined_train_loader = GraphDataLoader(list(zip(combined_train_graphs, combined_train_labels)), batch_size=best_hyperparams['batch_size'], shuffle=True, collate_fn=collate, num_workers=4)\n",
    "\n",
    "    \n",
    "    # 2. Retraining with best hyperparameters (on a larger train and val set)\n",
    "    print(\"Retraining with best hyperparameters...\")\n",
    "    model = GraphClsGGNN(annotation_size=74, out_feats=best_hyperparams['out_feats'], n_steps=best_hyperparams['n_steps'], n_etypes=1, num_cls=2, dropout_rate=best_hyperparams['dropout_rate']).to(device)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Reset model parameters before retraining\n",
    "    model.reset_parameters()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_hyperparams['lr'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "    training_pipeline = TrainingPipeline(device)\n",
    "    training_pipeline.train_and_evaluate(model, train_loader, val_loader, optimizer, criterion, early_stopping, 50, plot_curves=True)\n",
    "    \n",
    "    # Before final training on the combined train and val dataset, reset the model again\n",
    "    model.reset_parameters()\n",
    "    print(\"\")\n",
    "    \n",
    "    # 3. Final training on the combined train and val dataset\n",
    "    print(\"Final training on the combined train and val dataset...\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_hyperparams['lr'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    early_stopping = EarlyStopping(patience=10, verbose=True, delta=0.001, path='final_model.pt')\n",
    "    training_pipeline.train_and_evaluate(model, combined_train_loader, None, optimizer, criterion, early_stopping, 100, plot_curves=True)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Evaluation on the test set\n",
    "    print(\"Evaluating on the test set...\")\n",
    "    training_pipeline.evaluate_on_test(model, test_loader, criterion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6546.501517,
   "end_time": "2024-03-11T19:18:01.562383",
   "environment_variables": {},
   "exception": null,
   "input_path": "gnn_ggnn_optuna_v_final2.ipynb",
   "output_path": "gnn_ggnn_optuna_v_final2.ipynb",
   "parameters": {},
   "start_time": "2024-03-11T17:28:55.060866",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}