{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d10b4903",
   "metadata": {
    "papermill": {
     "duration": 0.010169,
     "end_time": "2024-04-27T22:00:39.623964",
     "exception": false,
     "start_time": "2024-04-27T22:00:39.613795",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Graph Neural Network for Molecular Interaction Prediction\n",
    "### SAGEConv\n",
    "\n",
    "This Jupyter Notebook outlines the process for training a Graph Neural Network (GNN) model to predict molecular interactions using the SAGEConv architecture. The goal of this project is to leverage the inherent graph structure of molecules for effective prediction of binding to RNA, a crucial factor in drug discovery and biological research.\n",
    "\n",
    "Each run of this notebook represents a distinct experiment with specified hyperparameters and configurations. Results and models from each run are saved separately for comparative analysis to ensure the reproducibility and statistical significance of our findings.\n",
    "\n",
    "### Notebook Details:\n",
    "\n",
    "- **Objective**: Predict molecular interactions with RNA using GNN.\n",
    "- **Model Architecture**: SAGEConv from the Deep Graph Library (DGL).\n",
    "- **Data Source**: Preprocessed molecular datasets.\n",
    "- **Run Number**: This notebook facilitates multiple runs. Specific details for each run, including the random state and run number, are set at the beginning to ensure reproducibility.\n",
    "\n",
    "Before executing the notebook, please adjust the `RANDOM_STATE` and `RUN_NUMBER` variables at the top of the notebook to reflect the specific experiment being conducted. This setup ensures each run's outputs are unique and traceable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0317277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:40.468347Z",
     "iopub.status.busy": "2024-04-27T22:00:40.468070Z",
     "iopub.status.idle": "2024-04-27T22:00:40.483789Z",
     "shell.execute_reply": "2024-04-27T22:00:40.482446Z"
    },
    "papermill": {
     "duration": 0.69087,
     "end_time": "2024-04-27T22:00:40.486507",
     "exception": false,
     "start_time": "2024-04-27T22:00:39.795637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis script implements a Graph Neural Network (GNN) using the SAGEConv architecture\\nfor the purpose of predicting molecular interactions. The implementation leverages\\nthe Deep Graph Library (DGL) for constructing and manipulating graphs, as well as\\nOptuna for hyperparameter optimization. The model includes features such as dropout,\\nearly stopping, and gradient scaling for improved training stability and performance.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script implements a Graph Neural Network (GNN) using the SAGEConv architecture\n",
    "for the purpose of predicting molecular interactions. The implementation leverages\n",
    "the Deep Graph Library (DGL) for constructing and manipulating graphs, as well as\n",
    "Optuna for hyperparameter optimization. The model includes features such as dropout,\n",
    "early stopping, and gradient scaling for improved training stability and performance.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7e4d4fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:40.825143Z",
     "iopub.status.busy": "2024-04-27T22:00:40.824674Z",
     "iopub.status.idle": "2024-04-27T22:00:43.009622Z",
     "shell.execute_reply": "2024-04-27T22:00:43.008778Z"
    },
    "papermill": {
     "duration": 2.234946,
     "end_time": "2024-04-27T22:00:43.013909",
     "exception": false,
     "start_time": "2024-04-27T22:00:40.778963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the random seed and run number at the top for reproducibility and to \n",
    "# differentiate runs\n",
    "RANDOM_STATE = 420  # Change for each run if needed\n",
    "RUN_NUMBER = 1  # Change for each run\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import dgl\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "dgl.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b83490",
   "metadata": {
    "papermill": {
     "duration": 0.063162,
     "end_time": "2024-04-27T22:00:43.088770",
     "exception": false,
     "start_time": "2024-04-27T22:00:43.025608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a498492d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:43.141192Z",
     "iopub.status.busy": "2024-04-27T22:00:43.140641Z",
     "iopub.status.idle": "2024-04-27T22:00:44.033015Z",
     "shell.execute_reply": "2024-04-27T22:00:44.031260Z"
    },
    "papermill": {
     "duration": 0.921864,
     "end_time": "2024-04-27T22:00:44.037043",
     "exception": false,
     "start_time": "2024-04-27T22:00:43.115179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from dgl.nn import SAGEConv, GlobalAttentionPooling\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from torch.cuda.amp import GradScaler, autocast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb8c489",
   "metadata": {
    "papermill": {
     "duration": 0.007389,
     "end_time": "2024-04-27T22:00:44.051942",
     "exception": false,
     "start_time": "2024-04-27T22:00:44.044553",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The SAGEConv Graph Neural Network Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a58dd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:44.071454Z",
     "iopub.status.busy": "2024-04-27T22:00:44.070919Z",
     "iopub.status.idle": "2024-04-27T22:00:44.081174Z",
     "shell.execute_reply": "2024-04-27T22:00:44.079623Z"
    },
    "papermill": {
     "duration": 0.023453,
     "end_time": "2024-04-27T22:00:44.084135",
     "exception": false,
     "start_time": "2024-04-27T22:00:44.060682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphClsSAGE(nn.Module):\n",
    "    \"\"\"\n",
    "    A Graph Neural Network (GNN) model using the GraphSAGE architecture for \n",
    "    graph classification.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_feats : int\n",
    "        The number of input features.\n",
    "    hidden_dim : int\n",
    "        The number of output features (hidden dimensions of each layer).\n",
    "    aggregator_type : str\n",
    "        The aggregator type ('mean', 'gcn', 'pool', 'lstm').\n",
    "    dropout_rate : float\n",
    "        The dropout rate for the input features.\n",
    "    num_cls : int\n",
    "        The number of classes for classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_feats, hidden_dim, aggregator_type, dropout_rate, num_cls):\n",
    "        super(GraphClsSAGE, self).__init__()\n",
    "        # The first SAGE layer\n",
    "        self.sage_1 = SAGEConv(\n",
    "            in_feats=in_feats,\n",
    "            out_feats=hidden_dim,\n",
    "            aggregator_type=aggregator_type,\n",
    "            feat_drop=dropout_rate,\n",
    "        )\n",
    "        \n",
    "        # The second SAGE layer\n",
    "        self.sage_2 = SAGEConv(\n",
    "            in_feats=hidden_dim,\n",
    "            out_feats=hidden_dim,\n",
    "            aggregator_type=aggregator_type,\n",
    "            feat_drop=dropout_rate,\n",
    "        )\n",
    "        \n",
    "        # Global attention pooling layer to aggregate node features to a graph-level feature\n",
    "        self.pooling = GlobalAttentionPooling(nn.Linear(hidden_dim, 1))\n",
    "        \n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(hidden_dim, num_cls)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, graph, feat):\n",
    "        # Apply dropout to input features\n",
    "        feat = self.dropout(feat)\n",
    "        \n",
    "        # Apply the first SAGE layer and then activation and dropout\n",
    "        h = self.sage_1(graph, feat)\n",
    "        h = F.relu(h)  # Apply activation here\n",
    "        h = self.dropout(h)  # Apply dropout after activation\n",
    "        \n",
    "        # Apply the second SAGE layer and then activation and dropout\n",
    "        h = self.sage_2(graph, h)\n",
    "        h = F.relu(h)  # Apply activation here\n",
    "        h = self.dropout(h)  # Apply dropout after activation\n",
    "        \n",
    "        # Aggregate node features to graph-level features using global attention pooling\n",
    "        hg = self.pooling(graph, h).squeeze()\n",
    "        \n",
    "        # Classify based on the graph-level representation\n",
    "        return self.fc(hg)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        for layer in self.children():\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb595eb",
   "metadata": {
    "papermill": {
     "duration": 0.006735,
     "end_time": "2024-04-27T22:00:44.110464",
     "exception": false,
     "start_time": "2024-04-27T22:00:44.103729",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Early Stopping Mechanism\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e877897",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:44.521413Z",
     "iopub.status.busy": "2024-04-27T22:00:44.521103Z",
     "iopub.status.idle": "2024-04-27T22:00:44.532955Z",
     "shell.execute_reply": "2024-04-27T22:00:44.531495Z"
    },
    "papermill": {
     "duration": 0.239151,
     "end_time": "2024-04-27T22:00:44.536124",
     "exception": false,
     "start_time": "2024-04-27T22:00:44.296973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if neither validation loss nor validation \n",
    "    accuracy improves after their respective patience levels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    patience_loss : int\n",
    "        How long to wait after last time validation loss improved.\n",
    "    patience_accuracy : int\n",
    "        How long to wait after last time validation accuracy improved.\n",
    "    verbose : bool\n",
    "        If True, prints a message for each validation metric improvement.\n",
    "    delta_loss : float\n",
    "        Minimum change in the validation loss to qualify as an improvement.\n",
    "    delta_accuracy : float\n",
    "        Minimum change in the validation accuracy to qualify as an improvement.\n",
    "    path : str\n",
    "        The file path where the model will be saved.\n",
    "    print_freq : int\n",
    "        The frequency at which to print messages during training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            patience_loss=10,\n",
    "            patience_accuracy=10,\n",
    "            verbose=True,\n",
    "            delta_loss=0.001,\n",
    "            delta_accuracy=0.001,\n",
    "            path='checkpoint.pt',\n",
    "            print_freq=5):\n",
    "        self.patience_loss = patience_loss\n",
    "        self.patience_accuracy = patience_accuracy\n",
    "        self.verbose = verbose\n",
    "        self.counter_loss = 0\n",
    "        self.counter_accuracy = 0\n",
    "        self.best_loss = np.Inf\n",
    "        self.best_accuracy = 0\n",
    "        self.early_stop = False\n",
    "        self.delta_loss = delta_loss\n",
    "        self.delta_accuracy = delta_accuracy\n",
    "        self.path = path\n",
    "        self.best_epoch = 0\n",
    "        self.print_freq = print_freq\n",
    "        \n",
    "    def __call__(self, val_loss, val_accuracy, model, epoch):\n",
    "        improved_loss = False\n",
    "        improved_accuracy = False\n",
    "        \n",
    "        # Check improvement for loss\n",
    "        if val_loss < self.best_loss - self.delta_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter_loss = 0\n",
    "            improved_loss = True\n",
    "        else:\n",
    "            self.counter_loss += 1\n",
    "        \n",
    "        # Check improvement for accuracy\n",
    "        if val_accuracy > self.best_accuracy + self.delta_accuracy:\n",
    "            self.best_accuracy = val_accuracy\n",
    "            self.counter_accuracy = 0\n",
    "            improved_accuracy = True\n",
    "        else:\n",
    "            self.counter_accuracy += 1\n",
    "        \n",
    "        # Save checkpoint if either metric improved\n",
    "        if improved_loss or improved_accuracy:\n",
    "            self.save_checkpoint(val_loss, val_accuracy, model)\n",
    "            self.best_epoch = epoch\n",
    "        \n",
    "        if self.verbose and (improved_loss or improved_accuracy):\n",
    "            print(f\"Improvement at epoch {epoch}: Loss = {val_loss}, \"\n",
    "                f\"Accuracy = {val_accuracy}\")\n",
    "        \n",
    "        # Determine if early stopping is triggered\n",
    "        if self.counter_loss >= self.patience_loss and \\\n",
    "            self.counter_accuracy >= self.patience_accuracy:\n",
    "            self.early_stop = True\n",
    "            if self.verbose:\n",
    "                print(\"Early stopping triggered\")\n",
    "                \n",
    "    def save_checkpoint(self, val_loss, val_accuracy, model):\n",
    "        # Implement saving logic based on your requirements\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        if self.verbose:\n",
    "            print(f\"Checkpoint saved: Loss = {val_loss}, \"\n",
    "                  f\"Accuracy = {val_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce54fd",
   "metadata": {
    "papermill": {
     "duration": 0.00644,
     "end_time": "2024-04-27T22:00:44.630940",
     "exception": false,
     "start_time": "2024-04-27T22:00:44.624500",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The Collate Function for DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccd53d1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:45.390812Z",
     "iopub.status.busy": "2024-04-27T22:00:45.390490Z",
     "iopub.status.idle": "2024-04-27T22:00:45.397359Z",
     "shell.execute_reply": "2024-04-27T22:00:45.395817Z"
    },
    "papermill": {
     "duration": 0.019052,
     "end_time": "2024-04-27T22:00:45.400974",
     "exception": false,
     "start_time": "2024-04-27T22:00:45.381922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    \"\"\"\n",
    "    Function to collate samples into a batch for the GraphDataLoader.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    samples : list\n",
    "        A list of tuples of the form (graph, label).\n",
    "    \"\"\"\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return batched_graph, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab936a2f",
   "metadata": {
    "papermill": {
     "duration": 0.008425,
     "end_time": "2024-04-27T22:00:45.418118",
     "exception": false,
     "start_time": "2024-04-27T22:00:45.409693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training and Evaluation Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a24ac7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:45.662641Z",
     "iopub.status.busy": "2024-04-27T22:00:45.662232Z",
     "iopub.status.idle": "2024-04-27T22:00:45.689171Z",
     "shell.execute_reply": "2024-04-27T22:00:45.687550Z"
    },
    "papermill": {
     "duration": 0.265733,
     "end_time": "2024-04-27T22:00:45.692400",
     "exception": false,
     "start_time": "2024-04-27T22:00:45.426667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainingPipeline:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "    def train_and_evaluate(\n",
    "            self,\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            early_stopping,\n",
    "            num_epochs,\n",
    "            plot_curves=False,\n",
    "            accumulation_steps=2):\n",
    "        train_losses, val_losses = [], []\n",
    "        scaler = GradScaler()  # Initialize the gradient scaler\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            optimizer.zero_grad()  # Initialize gradients to zero\n",
    "\n",
    "            for batch_idx, (batched_graph, labels) in enumerate(train_loader):\n",
    "                batched_graph, labels = batched_graph.to(\n",
    "                    self.device), labels.to(self.device)\n",
    "\n",
    "                with autocast():  # Enable automatic mixed precision\n",
    "                    logits = model(\n",
    "                        batched_graph, batched_graph.ndata['h'].float())\n",
    "                    loss = criterion(logits, labels) / \\\n",
    "                        accumulation_steps  # Scale loss\n",
    "\n",
    "                # Scale the loss and call backward to propagate gradients\n",
    "                scaler.scale(loss).backward()\n",
    "                # Correct scaling for logging purposes\n",
    "                train_loss += loss.item() * accumulation_steps\n",
    "\n",
    "                if (batch_idx + 1) % accumulation_steps == 0 or \\\n",
    "                        batch_idx == len(train_loader) - 1:\n",
    "                    # Perform optimizer step using scaled gradients\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()  # Update the scaler for the next iteration\n",
    "                    optimizer.zero_grad()  # Initialize gradients to zero\n",
    "\n",
    "            train_loss /= len(train_loader)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            val_loss = 0.0\n",
    "            val_accuracy = 0.0\n",
    "            val_correct = 0\n",
    "            total = 0\n",
    "            if val_loader is not None:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_correct = 0\n",
    "                    total = 0\n",
    "                    for batched_graph, labels in val_loader:\n",
    "                        batched_graph, labels = batched_graph.to(\n",
    "                            self.device), labels.to(self.device)\n",
    "                        with autocast():  # Enable automatic mixed precision\n",
    "                            logits = model(\n",
    "                                batched_graph, batched_graph.ndata['h'].float()\n",
    "                            )\n",
    "                            loss = criterion(logits, labels)\n",
    "                        val_loss += loss.item()\n",
    "                        _, predicted = torch.max(logits.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "                    val_loss /= len(val_loader)\n",
    "                    val_losses.append(val_loss)\n",
    "                    val_accuracy = val_correct / total\n",
    "                    \n",
    "                    # here will be early stopping\n",
    "                    if early_stopping:\n",
    "                        early_stopping(val_loss, val_accuracy, model, epoch + 1)\n",
    "                        if early_stopping.early_stop:\n",
    "                            print(\n",
    "                                f\"Early stopping triggered\"\n",
    "                                f\"at epoch {epoch + 1}\")\n",
    "                            break\n",
    "\n",
    "                if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                    print(\n",
    "                        f'Epoch {epoch + 1}/{num_epochs} - '\n",
    "                        f'Train Loss: {train_loss:.4f}, '\n",
    "                        f'Val Loss: {val_loss:.4f} '\n",
    "                        f'| Val accuracy: {100 * val_accuracy:.2f}%')\n",
    "\n",
    "        if plot_curves and val_loader is not None:\n",
    "            self.plot_loss_curves(train_losses, val_losses)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_loss_curves(train_losses, val_losses):\n",
    "        sns.set(style=\"whitegrid\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "        plt.plot(epochs, train_losses, label='Training Loss')\n",
    "        plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss of SAGEConv')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'loss_curves_{RUN_NUMBER}.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate_on_test(self, model, test_loader, criterion, run_id):\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batched_graph, labels in test_loader:\n",
    "                batched_graph, labels = batched_graph.to(self.device), labels.to(self.device)\n",
    "                logits = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "                loss = criterion(logits, labels)\n",
    "                test_loss += loss.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds)\n",
    "        recall = recall_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "        # Calculate and save confusion matrix\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot()\n",
    "        plt.savefig(f'confusion_matrix_{run_id}.png', dpi=300)\n",
    "\n",
    "        # Append results to CSV including added metrics\n",
    "        results_df = pd.DataFrame({\n",
    "            'Run ID': [run_id],\n",
    "            'Test Loss': [test_loss],\n",
    "            'Accuracy': [accuracy],\n",
    "            'Precision': [precision],\n",
    "            'Recall': [recall],\n",
    "            'F1-Score': [f1],\n",
    "            'ROC-AUC': [roc_auc]\n",
    "        })\n",
    "        results_df.to_csv('test_results_SageConv.csv', mode='a', index=False, \n",
    "                        header=not os.path.exists('test_results_SageConv.csv'))\n",
    "\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "        print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}, ROC-AUC: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80678915",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hyperparameter Optimization Using Optuna\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066cb2c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T18:01:39.453471Z",
     "iopub.status.busy": "2024-04-21T18:01:39.453212Z",
     "iopub.status.idle": "2024-04-21T18:01:39.465513Z",
     "shell.execute_reply": "2024-04-21T18:01:39.464272Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HyperparameterOptimizer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            device,\n",
    "            subset_train_graphs,\n",
    "            subset_train_labels,\n",
    "            subset_val_graphs,\n",
    "            subset_val_labels,\n",
    "            num_trials,\n",
    "            num_epochs):\n",
    "        self.device = device\n",
    "        self.subset_train_graphs = subset_train_graphs\n",
    "        self.subset_train_labels = subset_train_labels\n",
    "        self.subset_val_graphs = subset_val_graphs\n",
    "        self.subset_val_labels = subset_val_labels\n",
    "        self.num_trials = num_trials\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def objective(self, trial):\n",
    "        # Adjusting the hyperparameters for GraphSAGE model\n",
    "        in_feats = 74  # Assuming this is fixed for your input features\n",
    "        hidden_dim = trial.suggest_int('hidden_dim', 16, 256)\n",
    "        aggregator_type = trial.suggest_categorical('aggregator_type', ['mean', 'gcn', 'pool', 'lstm'])\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "        lr = trial.suggest_float('lr', 1e-4, 1e-1, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [64, 128, 256, 512])\n",
    "\n",
    "        # Create the GraphSAGE model, optimizer, and loaders\n",
    "        model = GraphClsSAGE(\n",
    "            in_feats=in_feats,\n",
    "            hidden_dim=hidden_dim,\n",
    "            aggregator_type=aggregator_type,\n",
    "            dropout_rate=dropout_rate,\n",
    "            num_cls=2,  # Assuming binary classification\n",
    "        ).to(self.device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        train_loader = GraphDataLoader(\n",
    "            list(zip(self.subset_train_graphs, self.subset_train_labels)),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate,\n",
    "            num_workers=8)\n",
    "        val_loader = GraphDataLoader(\n",
    "            list(zip(self.subset_val_graphs, self.subset_val_labels)),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=collate,\n",
    "            num_workers=8)\n",
    "\n",
    "        # Training loop with pruning\n",
    "        model.train()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            model.train()  # Ensure the model is in training mode\n",
    "            for batched_graph, labels in train_loader:\n",
    "                batched_graph, labels = batched_graph.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "                loss = criterion(logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()  # Switch to evaluation mode for validation\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batched_graph, labels in val_loader:\n",
    "                    batched_graph, labels = batched_graph.to(self.device), labels.to(self.device)\n",
    "                    logits = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "                    loss = criterion(logits, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            trial.report(val_loss, epoch)\n",
    "\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\"Run the hyperparameter optimization.\"\"\"\n",
    "        study = optuna.create_study(direction='minimize', pruner=MedianPruner())\n",
    "        study.optimize(self.objective, n_trials=self.num_trials)\n",
    "\n",
    "        best_hyperparams = study.best_trial.params\n",
    "        with open(f'sage_best_hyperparams_run_{RUN_NUMBER}.json', 'w') as f:\n",
    "            json.dump(best_hyperparams, f)\n",
    "        print(f\"Best hyperparameters are {best_hyperparams}.\")\n",
    "        print(\"Best hyperparameters saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd25dce",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Main Training Loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afadd9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T18:01:39.776968Z",
     "iopub.status.busy": "2024-04-21T18:01:39.776596Z",
     "iopub.status.idle": "2024-04-21T18:01:39.805079Z",
     "shell.execute_reply": "2024-04-21T18:01:39.803541Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a35108",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T18:01:39.987106Z",
     "iopub.status.busy": "2024-04-21T18:01:39.986823Z",
     "iopub.status.idle": "2024-04-21T18:01:53.363681Z",
     "shell.execute_reply": "2024-04-21T18:01:53.361928Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data and prepare for training\n",
    "graphs, labels_dict = dgl.load_graphs(\"../../data_mvi/graphs.bin\")\n",
    "\n",
    "# Directly extract labels tensor from labels_dict\n",
    "labels_tensor = labels_dict['labels']\n",
    "labels_tensor = labels_tensor.squeeze()\n",
    "\n",
    "# Convert tensor to numpy array for compatibility with sklearn\n",
    "labels_numpy = labels_tensor.numpy()\n",
    "\n",
    "# Split dataset train, test\n",
    "train_indices, test_indices, train_labels, test_labels = train_test_split(\n",
    "    range(labels_dict['labels'].size(0)), labels_numpy, test_size=0.2, stratify=labels_numpy,\n",
    "    random_state=RANDOM_STATE)\n",
    "\n",
    "# Split dataset train, validation\n",
    "train_indices, val_indices, train_labels, val_labels = train_test_split(\n",
    "    train_indices, train_labels, test_size=0.2, stratify=train_labels,\n",
    "    random_state=RANDOM_STATE)\n",
    "\n",
    "# Placeholder for data loading. Replace this with your actual data loading\n",
    "# code.\n",
    "train_graphs = [graphs[i] for i in train_indices]\n",
    "test_graphs = [graphs[i] for i in test_indices]\n",
    "val_graphs = [graphs[i] for i in val_indices]\n",
    "\n",
    "subset_train_indices = np.random.choice(\n",
    "    len(train_graphs), size=int(len(train_graphs) * 0.2), replace=False)\n",
    "subset_train_graphs = [train_graphs[i] for i in subset_train_indices]\n",
    "subset_train_labels = train_labels[subset_train_indices]\n",
    "\n",
    "subset_val_indices = np.random.choice(\n",
    "    len(val_graphs), size=int(len(val_graphs) * 0.2), replace=False)\n",
    "subset_val_graphs = [val_graphs[i] for i in subset_val_indices]\n",
    "subset_val_labels = val_labels[subset_val_indices]\n",
    "\n",
    "# Combine train and validation graphs and labels for retraining\n",
    "combined_train_graphs = train_graphs + val_graphs\n",
    "combined_train_labels = np.concatenate((train_labels, val_labels))\n",
    "\n",
    "# annouce the start of the project\n",
    "print(\"Starting the project...\")\n",
    "print(\"\")\n",
    "\n",
    "# annouce the start of the data loading\n",
    "print(\"Starting data loading...\")\n",
    "print(\n",
    "    f'Train: {len(train_graphs)}, Validation: {len(val_graphs)}, '\n",
    "    f'Test: {len(test_graphs)}, \\nSubset Train: {len(subset_train_graphs)}, '\n",
    "    f'Subset Val: {len(subset_val_graphs)}'\n",
    ")\n",
    "print(\"\")\n",
    "print(\"Completed data loading.\")\n",
    "print(\"\")\n",
    "sys.stdout.flush()  # Force flushing of the buffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c83564",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Perform Hyperparameter Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a2c949",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T18:01:53.970869Z",
     "iopub.status.busy": "2024-04-21T18:01:53.970574Z",
     "iopub.status.idle": "2024-04-21T18:17:22.195871Z",
     "shell.execute_reply": "2024-04-21T18:17:22.194860Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Hyperparameter Optimization on a subset of the data\n",
    "print(\"Starting hyperparameter optimization...\")\n",
    "sys.stdout.flush()\n",
    "print(\"\")\n",
    "\n",
    "# Specify the number of trials and epochs for hyperparameter optimization\n",
    "optimizer = HyperparameterOptimizer(\n",
    "    device,\n",
    "    subset_train_graphs,\n",
    "    subset_train_labels,\n",
    "    subset_val_graphs,\n",
    "    subset_val_labels,\n",
    "    num_trials=20,\n",
    "    num_epochs=30)\n",
    "optimizer.optimize()\n",
    "print(\"Completed hyperparameter optimization.\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ccb06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T18:17:22.230022Z",
     "iopub.status.busy": "2024-04-21T18:17:22.229745Z",
     "iopub.status.idle": "2024-04-21T18:17:22.909401Z",
     "shell.execute_reply": "2024-04-21T18:17:22.907595Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the best hyperparameters\n",
    "with open(f'sage_best_hyperparams_run_{RUN_NUMBER}.json', 'r') as f:\n",
    "    best_hyperparams = json.load(f)\n",
    "\n",
    "train_loader = GraphDataLoader(list(zip(train_graphs,\n",
    "                                        train_labels)),\n",
    "                                batch_size=best_hyperparams['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=collate,\n",
    "                                num_workers=8)\n",
    "val_loader = GraphDataLoader(list(zip(val_graphs,\n",
    "                                        val_labels)),\n",
    "                                batch_size=best_hyperparams['batch_size'],\n",
    "                                shuffle=False,\n",
    "                                collate_fn=collate,\n",
    "                                num_workers=8)\n",
    "test_loader = GraphDataLoader(list(zip(test_graphs,\n",
    "                                        test_labels)),\n",
    "                                batch_size=best_hyperparams['batch_size'],\n",
    "                                shuffle=False,\n",
    "                                collate_fn=collate,\n",
    "                                num_workers=8)\n",
    "combined_train_loader = GraphDataLoader(\n",
    "    list(\n",
    "        zip(\n",
    "            combined_train_graphs,\n",
    "            combined_train_labels)),\n",
    "    batch_size=best_hyperparams['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate,\n",
    "    num_workers=8)\n",
    "print(\"Data loaders created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c812c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T18:17:22.979243Z",
     "iopub.status.busy": "2024-04-21T18:17:22.978855Z",
     "iopub.status.idle": "2024-04-21T18:26:47.518177Z",
     "shell.execute_reply": "2024-04-21T18:26:47.516617Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Retraining with best hyperparameters (on a larger train and val set)\n",
    "print(\"Retraining with best hyperparameters...\")\n",
    "model = GraphClsSAGE(\n",
    "    in_feats=74,  # This should match the input feature size of your dataset\n",
    "    hidden_dim=best_hyperparams['hidden_dim'],\n",
    "    aggregator_type=best_hyperparams['aggregator_type'],\n",
    "    dropout_rate=best_hyperparams['dropout_rate'],\n",
    "    num_cls=2  # Assuming binary classification\n",
    ").to(device)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Reset model weights and biases parameters before retraining\n",
    "model.reset_parameters()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_hyperparams['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "early_stopping = EarlyStopping(patience_loss=10, patience_accuracy=10, \n",
    "                               verbose=False, delta_loss=0.001, \n",
    "                               delta_accuracy=0.001, path='checkpoint.pt', \n",
    "                               print_freq=5)\n",
    "\n",
    "training_pipeline = TrainingPipeline(device)\n",
    "\n",
    "training_pipeline.train_and_evaluate(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    early_stopping,\n",
    "    300,\n",
    "    plot_curves=True)\n",
    "optimal_epoch = early_stopping.best_epoch\n",
    "\n",
    "\n",
    "print(\"Completed training.\")\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ad1392",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train Model with Best Hyperparameters on whole train set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5fa57a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T18:26:47.734735Z",
     "iopub.status.busy": "2024-04-21T18:26:47.734448Z",
     "iopub.status.idle": "2024-04-21T18:26:47.741541Z",
     "shell.execute_reply": "2024-04-21T18:26:47.740097Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Before final training on the combined train and val dataset, reset the\n",
    "# model weights and biases again\n",
    "model.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c050de18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T18:26:48.378820Z",
     "iopub.status.busy": "2024-04-21T18:26:48.378535Z",
     "iopub.status.idle": "2024-04-21T18:34:43.046766Z",
     "shell.execute_reply": "2024-04-21T18:34:43.045577Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Final training on the combined train and val dataset with best \n",
    "# hyperparameters\n",
    "print(\"Final training on the combined train and val dataset...\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_hyperparams['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "training_pipeline.train_and_evaluate(\n",
    "    model,\n",
    "    combined_train_loader,\n",
    "    None,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    None,\n",
    "    optimal_epoch,\n",
    "    plot_curves=False)\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), f'sage_model_run_{RUN_NUMBER}.pt')\n",
    "\n",
    "print(\"Completed training.\")\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54041443",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluate the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41720c86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T18:34:43.534988Z",
     "iopub.status.busy": "2024-04-21T18:34:43.534705Z",
     "iopub.status.idle": "2024-04-21T18:34:45.576281Z",
     "shell.execute_reply": "2024-04-21T18:34:45.574810Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation on the test set\n",
    "print(\"Evaluating on the test set...\")\n",
    "training_pipeline.evaluate_on_test(model, test_loader, criterion, RUN_NUMBER)\n",
    "print(\"Completed evaluation.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7.502346,
   "end_time": "2024-04-27T22:00:45.916824",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/xfulop/mvi/diploma_project_gnn/sageconv/sageconv_cls_run_1.ipynb",
   "output_path": "/home/xfulop/mvi/diploma_project_gnn/sageconv/sageconv_cls_run_1.ipynb",
   "parameters": {},
   "start_time": "2024-04-27T22:00:38.414478",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}