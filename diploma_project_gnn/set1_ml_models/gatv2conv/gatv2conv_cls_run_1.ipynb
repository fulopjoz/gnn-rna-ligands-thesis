{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59963f9e",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [10]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb77659",
   "metadata": {
    "papermill": {
     "duration": 0.011631,
     "end_time": "2024-04-27T22:00:36.309034",
     "exception": false,
     "start_time": "2024-04-27T22:00:36.297403",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Graph Neural Network for Molecular Interaction Prediction\n",
    "\n",
    "This Jupyter Notebook outlines the process for training a Graph Neural Network (GNN) model to predict molecular interactions using the GATv2 architecture. The goal of this project is to leverage the inherent graph structure of molecules for effective prediction of binding to RNA, a crucial factor in drug discovery and biological research.\n",
    "\n",
    "Each run of this notebook represents a distinct experiment with specified hyperparameters and configurations. Results and models from each run are saved separately for comparative analysis to ensure the reproducibility and statistical significance of our findings.\n",
    "\n",
    "### Notebook Details:\n",
    "\n",
    "- **Objective**: Predict molecular interactions with RNA using GNN.\n",
    "- **Model Architecture**: GATv2Conv from the Deep Graph Library (DGL).\n",
    "- **Data Source**: Preprocessed molecular interaction datasets.\n",
    "- **Run Number**: This notebook facilitates multiple runs. Specific details for each run, including the random state and run number, are set at the beginning to ensure reproducibility.\n",
    "\n",
    "Before executing the notebook, please adjust the `RANDOM_STATE` and `RUN_NUMBER` variables at the top of the notebook to reflect the specific experiment being conducted. This setup ensures each run's outputs are unique and traceable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "282e28a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:36.329357Z",
     "iopub.status.busy": "2024-04-27T22:00:36.328954Z",
     "iopub.status.idle": "2024-04-27T22:00:36.347367Z",
     "shell.execute_reply": "2024-04-27T22:00:36.345702Z"
    },
    "papermill": {
     "duration": 0.033297,
     "end_time": "2024-04-27T22:00:36.351316",
     "exception": false,
     "start_time": "2024-04-27T22:00:36.318019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis script implements a Graph Neural Network (GNN) using the GATv2 architecture\\nfor the purpose of predicting molecular interactions. The implementation leverages\\nthe Deep Graph Library (DGL) for constructing and manipulating graphs, as well as\\nOptuna for hyperparameter optimization. The model includes features such as dropout,\\nearly stopping, and gradient scaling for improved training stability and performance.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script implements a Graph Neural Network (GNN) using the GATv2 architecture\n",
    "for the purpose of predicting molecular interactions. The implementation leverages\n",
    "the Deep Graph Library (DGL) for constructing and manipulating graphs, as well as\n",
    "Optuna for hyperparameter optimization. The model includes features such as dropout,\n",
    "early stopping, and gradient scaling for improved training stability and performance.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1f244f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:36.446251Z",
     "iopub.status.busy": "2024-04-27T22:00:36.445848Z",
     "iopub.status.idle": "2024-04-27T22:00:38.665504Z",
     "shell.execute_reply": "2024-04-27T22:00:38.664689Z"
    },
    "papermill": {
     "duration": 2.309654,
     "end_time": "2024-04-27T22:00:38.669754",
     "exception": false,
     "start_time": "2024-04-27T22:00:36.360100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the random seed and run number at the top for reproducibility and to differentiate runs\n",
    "RANDOM_STATE = 420  \n",
    "RUN_NUMBER = 1   \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import dgl\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "dgl.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15f3fdc",
   "metadata": {
    "papermill": {
     "duration": 0.283499,
     "end_time": "2024-04-27T22:00:38.964380",
     "exception": false,
     "start_time": "2024-04-27T22:00:38.680881",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e29631b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:39.285432Z",
     "iopub.status.busy": "2024-04-27T22:00:39.284954Z",
     "iopub.status.idle": "2024-04-27T22:00:40.174617Z",
     "shell.execute_reply": "2024-04-27T22:00:40.172706Z"
    },
    "papermill": {
     "duration": 1.002755,
     "end_time": "2024-04-27T22:00:40.179313",
     "exception": false,
     "start_time": "2024-04-27T22:00:39.176558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from dgl.nn import GATv2Conv, GlobalAttentionPooling\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from torch.cuda.amp import GradScaler, autocast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b346c89a",
   "metadata": {
    "papermill": {
     "duration": 0.278077,
     "end_time": "2024-04-27T22:00:40.653111",
     "exception": false,
     "start_time": "2024-04-27T22:00:40.375034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The Graph Neural Network Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7796afe2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:41.427209Z",
     "iopub.status.busy": "2024-04-27T22:00:41.426601Z",
     "iopub.status.idle": "2024-04-27T22:00:41.437269Z",
     "shell.execute_reply": "2024-04-27T22:00:41.436214Z"
    },
    "papermill": {
     "duration": 0.386857,
     "end_time": "2024-04-27T22:00:41.440349",
     "exception": false,
     "start_time": "2024-04-27T22:00:41.053492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphClsGATv2(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_feats, \n",
    "                 hidden_dim, \n",
    "                 num_heads, \n",
    "                 num_cls,\n",
    "                 negative_slope, \n",
    "                 dropout_rate, \n",
    "                 ):\n",
    "        super(GraphClsGATv2, self).__init__()\n",
    "        \n",
    "        self.layer1 = GATv2Conv(in_feats, \n",
    "                                hidden_dim, \n",
    "                                num_heads=num_heads, \n",
    "                                attn_drop=dropout_rate,\n",
    "                                feat_drop=dropout_rate, \n",
    "                                negative_slope=negative_slope,\n",
    "                                residual=True)\n",
    "        self.layer2 = GATv2Conv(hidden_dim * num_heads, \n",
    "                                hidden_dim, num_heads=1, \n",
    "                                attn_drop=dropout_rate,\n",
    "                                feat_drop=dropout_rate, \n",
    "                                negative_slope=negative_slope,\n",
    "                                residual=True)\n",
    "        \n",
    "        self.pooling = GlobalAttentionPooling(gate_nn=nn.Linear(hidden_dim, 1))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, num_cls)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(hidden_dim * num_heads)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = F.elu(self.layer1(g, h).flatten(1))\n",
    "        h = self.norm1(h)\n",
    "        h = F.elu(self.layer2(g, h).flatten(1))\n",
    "        h = self.norm2(h)\n",
    "        h = self.dropout(h)\n",
    "        \n",
    "        hg = self.pooling(g, h)\n",
    "        out = self.fc(hg)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.children():\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b853e3",
   "metadata": {
    "papermill": {
     "duration": 0.009275,
     "end_time": "2024-04-27T22:00:41.702557",
     "exception": false,
     "start_time": "2024-04-27T22:00:41.693282",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Early Stopping Mechanism\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dfe1a10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:41.722280Z",
     "iopub.status.busy": "2024-04-27T22:00:41.721997Z",
     "iopub.status.idle": "2024-04-27T22:00:41.735704Z",
     "shell.execute_reply": "2024-04-27T22:00:41.734239Z"
    },
    "papermill": {
     "duration": 0.027896,
     "end_time": "2024-04-27T22:00:41.739170",
     "exception": false,
     "start_time": "2024-04-27T22:00:41.711274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if neither validation loss nor validation \n",
    "    accuracy improves after their respective patience levels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    patience_loss : int\n",
    "        How long to wait after last time validation loss improved.\n",
    "    patience_accuracy : int\n",
    "        How long to wait after last time validation accuracy improved.\n",
    "    verbose : bool\n",
    "        If True, prints a message for each validation metric improvement.\n",
    "    delta_loss : float\n",
    "        Minimum change in the validation loss to qualify as an improvement.\n",
    "    delta_accuracy : float\n",
    "        Minimum change in the validation accuracy to qualify as an improvement.\n",
    "    path : str\n",
    "        The file path where the model will be saved.\n",
    "    print_freq : int\n",
    "        The frequency at which to print messages during training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            patience_loss=10,\n",
    "            patience_accuracy=10,\n",
    "            verbose=True,\n",
    "            delta_loss=0.001,\n",
    "            delta_accuracy=0.001,\n",
    "            path='checkpoint.pt',\n",
    "            print_freq=5):\n",
    "        self.patience_loss = patience_loss\n",
    "        self.patience_accuracy = patience_accuracy\n",
    "        self.verbose = verbose\n",
    "        self.counter_loss = 0\n",
    "        self.counter_accuracy = 0\n",
    "        self.best_loss = np.Inf\n",
    "        self.best_accuracy = 0\n",
    "        self.early_stop = False\n",
    "        self.delta_loss = delta_loss\n",
    "        self.delta_accuracy = delta_accuracy\n",
    "        self.path = path\n",
    "        self.best_epoch = 0\n",
    "        self.print_freq = print_freq\n",
    "        \n",
    "    def __call__(self, val_loss, val_accuracy, model, epoch):\n",
    "        improved_loss = False\n",
    "        improved_accuracy = False\n",
    "        \n",
    "        # Check improvement for loss\n",
    "        if val_loss < self.best_loss - self.delta_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter_loss = 0\n",
    "            improved_loss = True\n",
    "        else:\n",
    "            self.counter_loss += 1\n",
    "        \n",
    "        # Check improvement for accuracy\n",
    "        if val_accuracy > self.best_accuracy + self.delta_accuracy:\n",
    "            self.best_accuracy = val_accuracy\n",
    "            self.counter_accuracy = 0\n",
    "            improved_accuracy = True\n",
    "        else:\n",
    "            self.counter_accuracy += 1\n",
    "        \n",
    "        # Save checkpoint if either metric improved\n",
    "        if improved_loss or improved_accuracy:\n",
    "            self.save_checkpoint(val_loss, val_accuracy, model)\n",
    "            self.best_epoch = epoch\n",
    "        \n",
    "        if self.verbose and (improved_loss or improved_accuracy):\n",
    "            print(f\"Improvement at epoch {epoch}: Loss = {val_loss}, \"\n",
    "                f\"Accuracy = {val_accuracy}\")\n",
    "        \n",
    "        # Determine if early stopping is triggered\n",
    "        if self.counter_loss >= self.patience_loss and \\\n",
    "            self.counter_accuracy >= self.patience_accuracy:\n",
    "            self.early_stop = True\n",
    "            if self.verbose:\n",
    "                print(\"Early stopping triggered\")\n",
    "                \n",
    "    def save_checkpoint(self, val_loss, val_accuracy, model):\n",
    "        # Implement saving logic based on your requirements\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        if self.verbose:\n",
    "            print(f\"Checkpoint saved: Loss = {val_loss}, \"\n",
    "                  f\"Accuracy = {val_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd97b08",
   "metadata": {
    "papermill": {
     "duration": 0.129863,
     "end_time": "2024-04-27T22:00:41.923999",
     "exception": false,
     "start_time": "2024-04-27T22:00:41.794136",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The Collate Function for DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5a6c490",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:42.211431Z",
     "iopub.status.busy": "2024-04-27T22:00:42.211103Z",
     "iopub.status.idle": "2024-04-27T22:00:42.218049Z",
     "shell.execute_reply": "2024-04-27T22:00:42.216306Z"
    },
    "papermill": {
     "duration": 0.181187,
     "end_time": "2024-04-27T22:00:42.221285",
     "exception": false,
     "start_time": "2024-04-27T22:00:42.040098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    \"\"\"\n",
    "    Function to collate samples into a batch for the GraphDataLoader.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    samples : list\n",
    "        A list of tuples of the form (graph, label).\n",
    "    \"\"\"\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return batched_graph, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3e818c",
   "metadata": {
    "papermill": {
     "duration": 0.042341,
     "end_time": "2024-04-27T22:00:42.412599",
     "exception": false,
     "start_time": "2024-04-27T22:00:42.370258",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training and Evaluation Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "304cfbb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:42.753546Z",
     "iopub.status.busy": "2024-04-27T22:00:42.753094Z",
     "iopub.status.idle": "2024-04-27T22:00:42.791402Z",
     "shell.execute_reply": "2024-04-27T22:00:42.789780Z"
    },
    "papermill": {
     "duration": 0.295029,
     "end_time": "2024-04-27T22:00:42.795302",
     "exception": false,
     "start_time": "2024-04-27T22:00:42.500273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainingPipeline:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "    def train_and_evaluate(\n",
    "            self,\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            early_stopping,\n",
    "            num_epochs,\n",
    "            plot_curves=False,\n",
    "            accumulation_steps=2):\n",
    "        train_losses, val_losses = [], []\n",
    "        scaler = GradScaler()  # Initialize the gradient scaler\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            optimizer.zero_grad()  # Initialize gradients to zero\n",
    "\n",
    "            for batch_idx, (batched_graph, labels) in enumerate(train_loader):\n",
    "                batched_graph, labels = batched_graph.to(\n",
    "                    self.device), labels.to(self.device)\n",
    "\n",
    "                with autocast():  # Enable automatic mixed precision\n",
    "                    logits = model(\n",
    "                        batched_graph, batched_graph.ndata['h'].float())\n",
    "                    loss = criterion(logits, labels) / \\\n",
    "                        accumulation_steps  # Scale loss\n",
    "\n",
    "                # Scale the loss and call backward to propagate gradients\n",
    "                scaler.scale(loss).backward()\n",
    "                # Correct scaling for logging purposes\n",
    "                train_loss += loss.item() * accumulation_steps\n",
    "\n",
    "                if (batch_idx + 1) % accumulation_steps == 0 or \\\n",
    "                        batch_idx == len(train_loader) - 1:\n",
    "                    # Perform optimizer step using scaled gradients\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()  # Update the scaler for the next iteration\n",
    "                    optimizer.zero_grad()  # Initialize gradients to zero\n",
    "\n",
    "            train_loss /= len(train_loader)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            val_loss = 0.0\n",
    "            val_accuracy = 0.0\n",
    "            val_correct = 0\n",
    "            total = 0\n",
    "            if val_loader is not None:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_correct = 0\n",
    "                    total = 0\n",
    "                    for batched_graph, labels in val_loader:\n",
    "                        batched_graph, labels = batched_graph.to(\n",
    "                            self.device), labels.to(self.device)\n",
    "                        with autocast():  # Enable automatic mixed precision\n",
    "                            logits = model(\n",
    "                                batched_graph, batched_graph.ndata['h'].float()\n",
    "                            )\n",
    "                            loss = criterion(logits, labels)\n",
    "                        val_loss += loss.item()\n",
    "                        _, predicted = torch.max(logits.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "                    val_loss /= len(val_loader)\n",
    "                    val_losses.append(val_loss)\n",
    "                    val_accuracy = val_correct / total\n",
    "                    \n",
    "                    # here will be early stopping\n",
    "                    if early_stopping:\n",
    "                        early_stopping(val_loss, val_accuracy, model, epoch + 1)\n",
    "                        if early_stopping.early_stop:\n",
    "                            print(\n",
    "                                f\"Early stopping triggered\"\n",
    "                                f\"at epoch {epoch + 1}\")\n",
    "                            break\n",
    "\n",
    "                if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                    print(\n",
    "                        f'Epoch {epoch + 1}/{num_epochs} - '\n",
    "                        f'Train Loss: {train_loss:.4f}, '\n",
    "                        f'Val Loss: {val_loss:.4f} '\n",
    "                        f'| Val accuracy: {100 * val_accuracy:.2f}%')\n",
    "\n",
    "        if plot_curves and val_loader is not None:\n",
    "            self.plot_loss_curves(train_losses, val_losses)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_loss_curves(train_losses, val_losses):\n",
    "        sns.set(style=\"whitegrid\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "        plt.plot(epochs, train_losses, label='Training Loss')\n",
    "        plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss of GATv2Conv')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'loss_curves_{RUN_NUMBER}.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate_on_test(self, model, test_loader, criterion, run_id):\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batched_graph, labels in test_loader:\n",
    "                batched_graph, labels = batched_graph.to(self.device), labels.to(self.device)\n",
    "                logits = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "                loss = criterion(logits, labels)\n",
    "                test_loss += loss.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds)\n",
    "        recall = recall_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "        # Calculate and save confusion matrix\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot()\n",
    "        plt.savefig(f'confusion_matrix_{run_id}.png', dpi=300)\n",
    "\n",
    "        # Append results to CSV including added metrics\n",
    "        results_df = pd.DataFrame({\n",
    "            'Run ID': [run_id],\n",
    "            'Test Loss': [test_loss],\n",
    "            'Accuracy': [accuracy],\n",
    "            'Precision': [precision],\n",
    "            'Recall': [recall],\n",
    "            'F1-Score': [f1],\n",
    "            'ROC-AUC': [roc_auc]\n",
    "        })\n",
    "        results_df.to_csv('test_results_GATv2Conv.csv', mode='a', index=False, \n",
    "                        header=not os.path.exists('test_results_GATv2Conv.csv'))\n",
    "\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "        print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}, ROC-AUC: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24d6dd8",
   "metadata": {
    "papermill": {
     "duration": 0.069591,
     "end_time": "2024-04-27T22:00:42.879326",
     "exception": false,
     "start_time": "2024-04-27T22:00:42.809735",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hyperparameter Optimization Using Optuna\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c427d42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:42.899105Z",
     "iopub.status.busy": "2024-04-27T22:00:42.898518Z",
     "iopub.status.idle": "2024-04-27T22:00:42.924441Z",
     "shell.execute_reply": "2024-04-27T22:00:42.922723Z"
    },
    "papermill": {
     "duration": 0.040348,
     "end_time": "2024-04-27T22:00:42.928194",
     "exception": false,
     "start_time": "2024-04-27T22:00:42.887846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HyperparameterOptimizer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            device,\n",
    "            subset_train_graphs,\n",
    "            subset_train_labels,\n",
    "            subset_val_graphs,\n",
    "            subset_val_labels,\n",
    "            num_trials,\n",
    "            num_epochs):\n",
    "        self.device = device\n",
    "        self.subset_train_graphs = subset_train_graphs\n",
    "        self.subset_train_labels = subset_train_labels\n",
    "        self.subset_val_graphs = subset_val_graphs\n",
    "        self.subset_val_labels = subset_val_labels\n",
    "        self.num_trials = num_trials\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def objective(self, trial):\n",
    "        # Adjusting the hyperparameters for GATv2Conv\n",
    "        in_feats = 74  \n",
    "        hidden_dim = trial.suggest_int('hidden_dim', 16, 256)\n",
    "        num_heads = trial.suggest_categorical('num_heads', [1, 2, 3, 4, 5, \n",
    "                                                              6, 7, 8, 9, 10, \n",
    "                                                            12, 14, 16, 18, 20])\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5) \n",
    "        negative_slope = trial.suggest_float('negative_slope', 0.01, 0.2)\n",
    "        lr = trial.suggest_float('lr', 1e-4, 1e-1, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [64, 128, 256, 512])\n",
    "\n",
    "        # Create the model, optimizer, and loaders\n",
    "        model = GraphClsGATv2(\n",
    "            in_feats=in_feats,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout_rate=dropout_rate,\n",
    "            negative_slope=negative_slope,\n",
    "            num_cls=2,\n",
    "        ).to(self.device)\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        train_loader = GraphDataLoader(\n",
    "            list(zip(self.subset_train_graphs, self.subset_train_labels)),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate,\n",
    "            num_workers=8)\n",
    "        val_loader = GraphDataLoader(\n",
    "            list(zip(self.subset_val_graphs, self.subset_val_labels)),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=collate,\n",
    "            num_workers=8)\n",
    "\n",
    "        # Training loop with pruning\n",
    "        model.train()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for batched_graph, labels in train_loader:\n",
    "                batched_graph, labels = batched_graph.to(\n",
    "                    self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "                loss = criterion(logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Validation phase and report for pruning\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batched_graph, labels in val_loader:\n",
    "                    batched_graph, labels = batched_graph.to(\n",
    "                        self.device), labels.to(self.device)\n",
    "                    logits = model(\n",
    "                        batched_graph, batched_graph.ndata['h'].float())\n",
    "                    loss = criterion(logits, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            # Report intermediate value to the pruner\n",
    "            trial.report(val_loss, epoch)\n",
    "\n",
    "            if trial.should_prune():  # Handle pruning based on the \n",
    "                                      # intermediate value\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\"Run the hyperparameter optimization.\n",
    "        \n",
    "         Returns\n",
    "         -------\n",
    "         dict\n",
    "             The best hyperparameters found by the optimization.\n",
    "        \"\"\"\n",
    "        study = optuna.create_study(direction='minimize',\n",
    "                                    pruner=MedianPruner())\n",
    "        study.optimize(self.objective, n_trials=self.num_trials)\n",
    "\n",
    "        best_hyperparams = study.best_trial.params\n",
    "        with open(f'gatv2_best_hyperparams_run_{RUN_NUMBER}.json', 'w') as f:\n",
    "            json.dump(best_hyperparams, f)\n",
    "        print(f\"Best hyperparameters are {best_hyperparams}.\")\n",
    "        print(\"Best hyperparameters saved.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8363371",
   "metadata": {
    "papermill": {
     "duration": 0.008355,
     "end_time": "2024-04-27T22:00:42.945509",
     "exception": false,
     "start_time": "2024-04-27T22:00:42.937154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Main Training Loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4822e1b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:42.965750Z",
     "iopub.status.busy": "2024-04-27T22:00:42.965308Z",
     "iopub.status.idle": "2024-04-27T22:00:43.161412Z",
     "shell.execute_reply": "2024-04-27T22:00:43.160246Z"
    },
    "papermill": {
     "duration": 0.209277,
     "end_time": "2024-04-27T22:00:43.163303",
     "exception": false,
     "start_time": "2024-04-27T22:00:42.954026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b1859e",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f8b58bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:43.200747Z",
     "iopub.status.busy": "2024-04-27T22:00:43.200266Z",
     "iopub.status.idle": "2024-04-27T22:00:43.962387Z",
     "shell.execute_reply": "2024-04-27T22:00:43.960443Z"
    },
    "papermill": {
     "duration": 0.79849,
     "end_time": "2024-04-27T22:00:43.965759",
     "exception": true,
     "start_time": "2024-04-27T22:00:43.167269",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "DGLError",
     "evalue": "File ../../data_mvi/graphs.bin does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDGLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load data and prepare for training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m graphs, labels_dict \u001b[38;5;241m=\u001b[39m \u001b[43mdgl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_graphs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../data_mvi/graphs.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Directly extract labels tensor from labels_dict\u001b[39;00m\n\u001b[1;32m      5\u001b[0m labels_tensor \u001b[38;5;241m=\u001b[39m labels_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/gnn/lib/python3.8/site-packages/dgl/data/graph_serialize.py:186\u001b[0m, in \u001b[0;36mload_graphs\u001b[0;34m(filename, idx_list)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load graphs and optionally their labels from file saved by :func:`save_graphs`.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03mBesides loading from local files, DGL supports loading the graphs directly\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03msave_graphs\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# if it is local file, do some sanity check\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m \u001b[43mcheck_local_file_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m version \u001b[38;5;241m=\u001b[39m _CAPI_GetFileVersion(filename)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gnn/lib/python3.8/site-packages/dgl/data/graph_serialize.py:40\u001b[0m, in \u001b[0;36mcheck_local_file_exists\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_local_file_exists\u001b[39m(filename):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_local_path(filename) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(filename):\n\u001b[0;32m---> 40\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m DGLError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(filename))\n",
      "\u001b[0;31mDGLError\u001b[0m: File ../../data_mvi/graphs.bin does not exist."
     ]
    }
   ],
   "source": [
    "# Load data and prepare for training\n",
    "graphs, labels_dict = dgl.load_graphs(\"../../data_mvi/graphs.bin\")\n",
    "\n",
    "# Directly extract labels tensor from labels_dict\n",
    "labels_tensor = labels_dict['labels']\n",
    "labels_tensor = labels_tensor.squeeze()\n",
    "\n",
    "# Convert tensor to numpy array for compatibility with sklearn\n",
    "labels_numpy = labels_tensor.numpy()\n",
    "\n",
    "# Split dataset train, test\n",
    "train_indices, test_indices, train_labels, test_labels = train_test_split(\n",
    "    range(labels_dict['labels'].size(0)), labels_numpy, test_size=0.2, stratify=labels_numpy,\n",
    "    random_state=RANDOM_STATE)\n",
    "\n",
    "# Split dataset train, validation\n",
    "train_indices, val_indices, train_labels, val_labels = train_test_split(\n",
    "    train_indices, train_labels, test_size=0.2, stratify=train_labels,\n",
    "    random_state=RANDOM_STATE)\n",
    "\n",
    "# Placeholder for data loading. Replace this with your actual data loading\n",
    "# code.\n",
    "train_graphs = [graphs[i] for i in train_indices]\n",
    "test_graphs = [graphs[i] for i in test_indices]\n",
    "val_graphs = [graphs[i] for i in val_indices]\n",
    "\n",
    "subset_train_indices = np.random.choice(\n",
    "    len(train_graphs), size=int(len(train_graphs) * 0.2), replace=False)\n",
    "subset_train_graphs = [train_graphs[i] for i in subset_train_indices]\n",
    "subset_train_labels = train_labels[subset_train_indices]\n",
    "\n",
    "subset_val_indices = np.random.choice(\n",
    "    len(val_graphs), size=int(len(val_graphs) * 0.2), replace=False)\n",
    "subset_val_graphs = [val_graphs[i] for i in subset_val_indices]\n",
    "subset_val_labels = val_labels[subset_val_indices]\n",
    "\n",
    "# Combine train and validation graphs and labels for retraining\n",
    "combined_train_graphs = train_graphs + val_graphs\n",
    "combined_train_labels = np.concatenate((train_labels, val_labels))\n",
    "\n",
    "# annouce the start of the project\n",
    "print(\"Starting the project...\")\n",
    "print(\"\")\n",
    "\n",
    "# annouce the start of the data loading\n",
    "print(\"Starting data loading...\")\n",
    "print(\n",
    "    f'Train: {len(train_graphs)}, Validation: {len(val_graphs)}, '\n",
    "    f'Test: {len(test_graphs)}, \\nSubset Train: {len(subset_train_graphs)}, '\n",
    "    f'Subset Val: {len(subset_val_graphs)}'\n",
    ")\n",
    "print(\"\")\n",
    "print(\"Completed data loading.\")\n",
    "print(\"\")\n",
    "sys.stdout.flush()  # Force flushing of the buffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad2830f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Perform Hyperparameter Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd59b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T00:15:23.834628Z",
     "iopub.status.busy": "2024-04-21T00:15:23.834301Z",
     "iopub.status.idle": "2024-04-21T00:29:00.597780Z",
     "shell.execute_reply": "2024-04-21T00:29:00.596761Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Hyperparameter Optimization on a subset of the data\n",
    "print(\"Starting hyperparameter optimization...\")\n",
    "sys.stdout.flush()\n",
    "print(\"\")\n",
    "\n",
    "# Specify the number of trials and epochs for hyperparameter optimization\n",
    "optimizer = HyperparameterOptimizer(\n",
    "    device,\n",
    "    subset_train_graphs,\n",
    "    subset_train_labels,\n",
    "    subset_val_graphs,\n",
    "    subset_val_labels,\n",
    "    num_trials=20,\n",
    "    num_epochs=30)\n",
    "optimizer.optimize()\n",
    "print(\"Completed hyperparameter optimization.\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf95a9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T00:29:00.613044Z",
     "iopub.status.busy": "2024-04-21T00:29:00.612647Z",
     "iopub.status.idle": "2024-04-21T00:29:01.719160Z",
     "shell.execute_reply": "2024-04-21T00:29:01.717753Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the best hyperparameters\n",
    "with open(f'gatv2_best_hyperparams_run_{RUN_NUMBER}.json', 'r') as f:\n",
    "    best_hyperparams = json.load(f)\n",
    "\n",
    "train_loader = GraphDataLoader(list(zip(train_graphs,\n",
    "                                        train_labels)),\n",
    "                                batch_size=best_hyperparams['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=collate,\n",
    "                                num_workers=8)\n",
    "val_loader = GraphDataLoader(list(zip(val_graphs,\n",
    "                                        val_labels)),\n",
    "                                batch_size=best_hyperparams['batch_size'],\n",
    "                                shuffle=False,\n",
    "                                collate_fn=collate,\n",
    "                                num_workers=8)\n",
    "test_loader = GraphDataLoader(list(zip(test_graphs,\n",
    "                                        test_labels)),\n",
    "                                batch_size=best_hyperparams['batch_size'],\n",
    "                                shuffle=False,\n",
    "                                collate_fn=collate,\n",
    "                                num_workers=8)\n",
    "combined_train_loader = GraphDataLoader(\n",
    "    list(\n",
    "        zip(\n",
    "            combined_train_graphs,\n",
    "            combined_train_labels)),\n",
    "    batch_size=best_hyperparams['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate,\n",
    "    num_workers=8)\n",
    "print(\"Data loaders created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ec7c58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T00:29:01.869997Z",
     "iopub.status.busy": "2024-04-21T00:29:01.869706Z",
     "iopub.status.idle": "2024-04-21T00:29:01.877244Z",
     "shell.execute_reply": "2024-04-21T00:29:01.876114Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2c91d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T00:29:01.904393Z",
     "iopub.status.busy": "2024-04-21T00:29:01.904138Z",
     "iopub.status.idle": "2024-04-21T00:42:42.877224Z",
     "shell.execute_reply": "2024-04-21T00:42:42.875311Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Retraining with best hyperparameters (on a larger train and val set)\n",
    "print(\"Retraining with best hyperparameters...\")\n",
    "model = GraphClsGATv2(\n",
    "    in_feats=74,  # Adjust this based on your dataset\n",
    "    hidden_dim=best_hyperparams['hidden_dim'],\n",
    "    num_heads=best_hyperparams['num_heads'],\n",
    "    dropout_rate=best_hyperparams['dropout_rate'],\n",
    "    negative_slope=best_hyperparams['negative_slope'],\n",
    "    num_cls=2, \n",
    ").to(device)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Reset model weights and biases parameters before retraining\n",
    "model.reset_parameters()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_hyperparams['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "early_stopping = EarlyStopping(patience_loss=10, patience_accuracy=10, \n",
    "                               verbose=False, delta_loss=0.001, \n",
    "                               delta_accuracy=0.001, path='checkpoint.pt', \n",
    "                               print_freq=5)\n",
    "\n",
    "training_pipeline = TrainingPipeline(device)\n",
    "\n",
    "training_pipeline.train_and_evaluate(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    early_stopping,\n",
    "    300,\n",
    "    plot_curves=True)\n",
    "optimal_epoch = early_stopping.best_epoch\n",
    "\n",
    "# Before final training on the combined train and val dataset, reset the\n",
    "# model weights and biases again\n",
    "model.reset_parameters()\n",
    "print(\"Completed training.\")\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5046b2c7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Train Model with Best Hyperparameters on whole train set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6741ab49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T00:42:43.191336Z",
     "iopub.status.busy": "2024-04-21T00:42:43.191022Z",
     "iopub.status.idle": "2024-04-21T00:55:01.555308Z",
     "shell.execute_reply": "2024-04-21T00:55:01.553293Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Final training on the combined train and val dataset\n",
    "print(\"Final training on the combined train and val dataset...\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_hyperparams['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "training_pipeline.train_and_evaluate(\n",
    "    model,\n",
    "    combined_train_loader,\n",
    "    None,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    None,\n",
    "    optimal_epoch,\n",
    "    plot_curves=False)\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), f'gatv2_model_run_{RUN_NUMBER}.pt')\n",
    "\n",
    "print(\"Completed training.\")\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6dcb21",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Evaluate the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5235b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T00:55:02.011324Z",
     "iopub.status.busy": "2024-04-21T00:55:02.010811Z",
     "iopub.status.idle": "2024-04-21T00:55:04.046127Z",
     "shell.execute_reply": "2024-04-21T00:55:04.044383Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation on the test set\n",
    "print(\"Evaluating on the test set...\")\n",
    "training_pipeline.evaluate_on_test(model, test_loader, criterion, RUN_NUMBER)\n",
    "print(\"Completed evaluation.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.335517,
   "end_time": "2024-04-27T22:00:47.470773",
   "environment_variables": {},
   "exception": true,
   "input_path": "/home/xfulop/mvi/diploma_project_gnn/gatv2conv/gatv2conv_cls_run_1.ipynb",
   "output_path": "/home/xfulop/mvi/diploma_project_gnn/gatv2conv/gatv2conv_cls_run_1.ipynb",
   "parameters": {},
   "start_time": "2024-04-27T22:00:35.135256",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}