{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e292f6d",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [10]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd9ed87",
   "metadata": {
    "papermill": {
     "duration": 0.013443,
     "end_time": "2024-04-27T22:00:38.693841",
     "exception": false,
     "start_time": "2024-04-27T22:00:38.680398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Graph Neural Network for Molecular Interaction Prediction\n",
    "### GatedGraphConv\n",
    "\n",
    "This Jupyter Notebook outlines the process for training a Graph Neural Network (GNN) model to predict molecular interactions using the GatedGraphConv architecture. The goal of this project is to leverage the inherent graph structure of molecules for effective prediction of binding to RNA, a crucial factor in drug discovery and biological research.\n",
    "\n",
    "Each run of this notebook represents a distinct experiment with specified hyperparameters and configurations. Results and models from each run are saved separately for comparative analysis to ensure the reproducibility and statistical significance of our findings.\n",
    "\n",
    "### Notebook Details:\n",
    "\n",
    "- **Objective**: Predict molecular interactions with RNA using GNN.\n",
    "- **Model Architecture**: GatedGraphConv from the Deep Graph Library (DGL).\n",
    "- **Data Source**: Preprocessed molecular datasets.\n",
    "- **Run Number**: This notebook facilitates multiple runs. Specific details for each run, including the random state and run number, are set at the beginning to ensure reproducibility.\n",
    "\n",
    "Before executing the notebook, please adjust the `RANDOM_STATE` and `RUN_NUMBER` variables at the top of the notebook to reflect the specific experiment being conducted. This setup ensures each run's outputs are unique and traceable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06014963",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:38.773310Z",
     "iopub.status.busy": "2024-04-27T22:00:38.772861Z",
     "iopub.status.idle": "2024-04-27T22:00:38.785446Z",
     "shell.execute_reply": "2024-04-27T22:00:38.784290Z"
    },
    "papermill": {
     "duration": 0.019467,
     "end_time": "2024-04-27T22:00:38.787543",
     "exception": false,
     "start_time": "2024-04-27T22:00:38.768076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis script implements a Graph Neural Network (GNN) using the GatedGraphConv architecture\\nfor the purpose of predicting molecular interactions. The implementation leverages\\nthe Deep Graph Library (DGL) for constructing and manipulating graphs, as well as\\nOptuna for hyperparameter optimization. The model includes features such as dropout,\\nearly stopping, and gradient scaling for improved training stability and performance.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script implements a Graph Neural Network (GNN) using the GatedGraphConv architecture\n",
    "for the purpose of predicting molecular interactions. The implementation leverages\n",
    "the Deep Graph Library (DGL) for constructing and manipulating graphs, as well as\n",
    "Optuna for hyperparameter optimization. The model includes features such as dropout,\n",
    "early stopping, and gradient scaling for improved training stability and performance.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd42937",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:38.796070Z",
     "iopub.status.busy": "2024-04-27T22:00:38.795645Z",
     "iopub.status.idle": "2024-04-27T22:00:40.917495Z",
     "shell.execute_reply": "2024-04-27T22:00:40.916618Z"
    },
    "papermill": {
     "duration": 2.128858,
     "end_time": "2024-04-27T22:00:40.920100",
     "exception": false,
     "start_time": "2024-04-27T22:00:38.791242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the random seed and run number at the top for reproducibility and to differentiate runs\n",
    "RANDOM_STATE = 123  \n",
    "RUN_NUMBER = 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import dgl\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "dgl.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019dd00e",
   "metadata": {
    "papermill": {
     "duration": 0.049882,
     "end_time": "2024-04-27T22:00:40.974794",
     "exception": false,
     "start_time": "2024-04-27T22:00:40.924912",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10132d76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:41.374267Z",
     "iopub.status.busy": "2024-04-27T22:00:41.373570Z",
     "iopub.status.idle": "2024-04-27T22:00:42.265837Z",
     "shell.execute_reply": "2024-04-27T22:00:42.264289Z"
    },
    "papermill": {
     "duration": 0.944877,
     "end_time": "2024-04-27T22:00:42.269094",
     "exception": false,
     "start_time": "2024-04-27T22:00:41.324217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from dgl.nn import GatedGraphConv, GlobalAttentionPooling\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a182ddb2",
   "metadata": {
    "papermill": {
     "duration": 0.00375,
     "end_time": "2024-04-27T22:00:42.277563",
     "exception": false,
     "start_time": "2024-04-27T22:00:42.273813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The GatedGraphConv Graph Neural Network Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48149bfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:42.318365Z",
     "iopub.status.busy": "2024-04-27T22:00:42.317824Z",
     "iopub.status.idle": "2024-04-27T22:00:42.326540Z",
     "shell.execute_reply": "2024-04-27T22:00:42.325036Z"
    },
    "papermill": {
     "duration": 0.047157,
     "end_time": "2024-04-27T22:00:42.328330",
     "exception": false,
     "start_time": "2024-04-27T22:00:42.281173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphClsGGNN(nn.Module):\n",
    "    \"\"\"GGNN for graph classification.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            annotation_size,\n",
    "            hidden_dim,\n",
    "            n_steps,\n",
    "            n_etypes,\n",
    "            num_cls,\n",
    "            dropout_rate=0.5):\n",
    "        super(GraphClsGGNN, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.ggnn1 = GatedGraphConv(\n",
    "            annotation_size, hidden_dim, n_steps, n_etypes)\n",
    "        self.ggnn2 = GatedGraphConv(hidden_dim, hidden_dim, n_steps, n_etypes)\n",
    "        self.pooling = GlobalAttentionPooling(nn.Linear(hidden_dim, 1))\n",
    "        self.fc = nn.Linear(hidden_dim, num_cls)\n",
    "\n",
    "    def forward(self, graph, feat):\n",
    "        h = F.relu(self.ggnn1(graph, feat))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.ggnn2(graph, h))\n",
    "        hg = self.pooling(graph, h)\n",
    "        return self.fc(hg)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.children():\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a944d35",
   "metadata": {
    "papermill": {
     "duration": 0.178063,
     "end_time": "2024-04-27T22:00:42.636008",
     "exception": false,
     "start_time": "2024-04-27T22:00:42.457945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Early Stopping Mechanism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ebec631",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:42.987062Z",
     "iopub.status.busy": "2024-04-27T22:00:42.986759Z",
     "iopub.status.idle": "2024-04-27T22:00:43.002946Z",
     "shell.execute_reply": "2024-04-27T22:00:43.000541Z"
    },
    "papermill": {
     "duration": 0.231863,
     "end_time": "2024-04-27T22:00:43.007110",
     "exception": false,
     "start_time": "2024-04-27T22:00:42.775247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if neither validation loss nor validation \n",
    "    accuracy improves after their respective patience levels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    patience_loss : int\n",
    "        How long to wait after last time validation loss improved.\n",
    "    patience_accuracy : int\n",
    "        How long to wait after last time validation accuracy improved.\n",
    "    verbose : bool\n",
    "        If True, prints a message for each validation metric improvement.\n",
    "    delta_loss : float\n",
    "        Minimum change in the validation loss to qualify as an improvement.\n",
    "    delta_accuracy : float\n",
    "        Minimum change in the validation accuracy to qualify as an improvement.\n",
    "    path : str\n",
    "        The file path where the model will be saved.\n",
    "    print_freq : int\n",
    "        The frequency at which to print messages during training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            patience_loss=10,\n",
    "            patience_accuracy=10,\n",
    "            verbose=True,\n",
    "            delta_loss=0.001,\n",
    "            delta_accuracy=0.001,\n",
    "            path='checkpoint.pt',\n",
    "            print_freq=5):\n",
    "        self.patience_loss = patience_loss\n",
    "        self.patience_accuracy = patience_accuracy\n",
    "        self.verbose = verbose\n",
    "        self.counter_loss = 0\n",
    "        self.counter_accuracy = 0\n",
    "        self.best_loss = np.Inf\n",
    "        self.best_accuracy = 0\n",
    "        self.early_stop = False\n",
    "        self.delta_loss = delta_loss\n",
    "        self.delta_accuracy = delta_accuracy\n",
    "        self.path = path\n",
    "        self.best_epoch = 0\n",
    "        self.print_freq = print_freq\n",
    "        \n",
    "    def __call__(self, val_loss, val_accuracy, model, epoch):\n",
    "        improved_loss = False\n",
    "        improved_accuracy = False\n",
    "        \n",
    "        # Check improvement for loss\n",
    "        if val_loss < self.best_loss - self.delta_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter_loss = 0\n",
    "            improved_loss = True\n",
    "        else:\n",
    "            self.counter_loss += 1\n",
    "        \n",
    "        # Check improvement for accuracy\n",
    "        if val_accuracy > self.best_accuracy + self.delta_accuracy:\n",
    "            self.best_accuracy = val_accuracy\n",
    "            self.counter_accuracy = 0\n",
    "            improved_accuracy = True\n",
    "        else:\n",
    "            self.counter_accuracy += 1\n",
    "        \n",
    "        # Save checkpoint if either metric improved\n",
    "        if improved_loss or improved_accuracy:\n",
    "            self.save_checkpoint(val_loss, val_accuracy, model)\n",
    "            self.best_epoch = epoch\n",
    "        \n",
    "        if self.verbose and (improved_loss or improved_accuracy):\n",
    "            print(f\"Improvement at epoch {epoch}: Loss = {val_loss}, \"\n",
    "                f\"Accuracy = {val_accuracy}\")\n",
    "        \n",
    "        # Determine if early stopping is triggered\n",
    "        if self.counter_loss >= self.patience_loss and \\\n",
    "            self.counter_accuracy >= self.patience_accuracy:\n",
    "            self.early_stop = True\n",
    "            if self.verbose:\n",
    "                print(\"Early stopping triggered\")\n",
    "                \n",
    "    def save_checkpoint(self, val_loss, val_accuracy, model):\n",
    "        # Implement saving logic based on your requirements\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        if self.verbose:\n",
    "            print(f\"Checkpoint saved: Loss = {val_loss}, \"\n",
    "                  f\"Accuracy = {val_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c95fe6",
   "metadata": {
    "papermill": {
     "duration": 0.013199,
     "end_time": "2024-04-27T22:00:43.155419",
     "exception": false,
     "start_time": "2024-04-27T22:00:43.142220",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The Collate Function for DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "449d5754",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:43.174391Z",
     "iopub.status.busy": "2024-04-27T22:00:43.173877Z",
     "iopub.status.idle": "2024-04-27T22:00:43.180342Z",
     "shell.execute_reply": "2024-04-27T22:00:43.179316Z"
    },
    "papermill": {
     "duration": 0.016653,
     "end_time": "2024-04-27T22:00:43.182869",
     "exception": false,
     "start_time": "2024-04-27T22:00:43.166216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    \"\"\"\n",
    "    Function to collate samples into a batch for the GraphDataLoader.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    samples : list\n",
    "        A list of tuples of the form (graph, label).\n",
    "    \"\"\"\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return batched_graph, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e773b8ec",
   "metadata": {
    "papermill": {
     "duration": 0.00613,
     "end_time": "2024-04-27T22:00:43.195082",
     "exception": false,
     "start_time": "2024-04-27T22:00:43.188952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training and Evaluation Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97a1bf36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:43.208765Z",
     "iopub.status.busy": "2024-04-27T22:00:43.208462Z",
     "iopub.status.idle": "2024-04-27T22:00:43.226623Z",
     "shell.execute_reply": "2024-04-27T22:00:43.225761Z"
    },
    "papermill": {
     "duration": 0.028064,
     "end_time": "2024-04-27T22:00:43.229178",
     "exception": false,
     "start_time": "2024-04-27T22:00:43.201114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainingPipeline:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "    def train_and_evaluate(\n",
    "            self,\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            early_stopping,\n",
    "            num_epochs,\n",
    "            plot_curves=False,\n",
    "            accumulation_steps=2):\n",
    "        train_losses, val_losses = [], []\n",
    "        scaler = GradScaler()  # Initialize the gradient scaler\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            optimizer.zero_grad()  # Initialize gradients to zero\n",
    "\n",
    "            for batch_idx, (batched_graph, labels) in enumerate(train_loader):\n",
    "                batched_graph, labels = batched_graph.to(\n",
    "                    self.device), labels.to(self.device)\n",
    "\n",
    "                with autocast():  # Enable automatic mixed precision\n",
    "                    logits = model(\n",
    "                        batched_graph, batched_graph.ndata['h'].float())\n",
    "                    loss = criterion(logits, labels) / \\\n",
    "                        accumulation_steps  # Scale loss\n",
    "\n",
    "                # Scale the loss and call backward to propagate gradients\n",
    "                scaler.scale(loss).backward()\n",
    "                # Correct scaling for logging purposes\n",
    "                train_loss += loss.item() * accumulation_steps\n",
    "\n",
    "                if (batch_idx + 1) % accumulation_steps == 0 or \\\n",
    "                        batch_idx == len(train_loader) - 1:\n",
    "                    # Perform optimizer step using scaled gradients\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()  # Update the scaler for the next iteration\n",
    "                    optimizer.zero_grad()  # Initialize gradients to zero\n",
    "\n",
    "            train_loss /= len(train_loader)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            total = 0\n",
    "            if val_loader is not None:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_correct = 0\n",
    "                    total = 0\n",
    "                    for batched_graph, labels in val_loader:\n",
    "                        batched_graph, labels = batched_graph.to(\n",
    "                            self.device), labels.to(self.device)\n",
    "                        with autocast():  # Enable automatic mixed precision\n",
    "                            logits = model(\n",
    "                                batched_graph, batched_graph.ndata['h'].float()\n",
    "                            )\n",
    "                            loss = criterion(logits, labels)\n",
    "                        val_loss += loss.item()\n",
    "                        _, predicted = torch.max(logits.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "                    val_loss /= len(val_loader)\n",
    "                    val_losses.append(val_loss)\n",
    "                    val_accuracy = val_correct / total\n",
    "\n",
    "                    if early_stopping:\n",
    "                        early_stopping(val_loss, val_accuracy, model, epoch + 1)\n",
    "                        if early_stopping.early_stop:\n",
    "                            print(\n",
    "                                f\"Early stopping triggered\"\n",
    "                                f\"at epoch {epoch + 1}\")\n",
    "                            break\n",
    "\n",
    "                if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                    print(\n",
    "                        f'Epoch {epoch + 1}/{num_epochs} - '\n",
    "                        f'Train Loss: {train_loss:.4f}, '\n",
    "                        f'Val Loss: {val_loss:.4f} '\n",
    "                        f'| Val accuracy: {100 * val_accuracy:.2f}%')\n",
    "\n",
    "        if plot_curves and val_loader is not None:\n",
    "            self.plot_loss_curves(train_losses, val_losses)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_loss_curves(train_losses, val_losses):\n",
    "        sns.set(style=\"whitegrid\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "        plt.plot(epochs, train_losses, label='Training Loss')\n",
    "        plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss of GatedGraphConv')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'loss_curves_GatedGraphConv_{RUN_NUMBER}.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate_on_test(self, model, test_loader, criterion, run_id):\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batched_graph, labels in test_loader:\n",
    "                batched_graph, labels = batched_graph.to(self.device), labels.to(self.device)\n",
    "                logits = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "                loss = criterion(logits, labels)\n",
    "                test_loss += loss.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds)\n",
    "        recall = recall_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "        # Calculate and save confusion matrix\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot()\n",
    "        plt.savefig(f'confusion_matrix_{run_id}.png', dpi=300)\n",
    "\n",
    "        # Append results to CSV including added metrics\n",
    "        results_df = pd.DataFrame({\n",
    "            'Run ID': [run_id],\n",
    "            'Test Loss': [test_loss],\n",
    "            'Accuracy': [accuracy],\n",
    "            'Precision': [precision],\n",
    "            'Recall': [recall],\n",
    "            'F1-Score': [f1],\n",
    "            'ROC-AUC': [roc_auc]\n",
    "        })\n",
    "        results_df.to_csv('test_results_GatedGraphConv.csv', mode='a', index=False, \n",
    "                        header=not os.path.exists('test_results_GatedGraphConv.csv'))\n",
    "\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "        print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}, ROC-AUC: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0adc46a",
   "metadata": {
    "papermill": {
     "duration": 0.124029,
     "end_time": "2024-04-27T22:00:43.359465",
     "exception": false,
     "start_time": "2024-04-27T22:00:43.235436",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hyperparameter Optimization Using Optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c695cd7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:43.599508Z",
     "iopub.status.busy": "2024-04-27T22:00:43.599228Z",
     "iopub.status.idle": "2024-04-27T22:00:43.612112Z",
     "shell.execute_reply": "2024-04-27T22:00:43.610695Z"
    },
    "papermill": {
     "duration": 0.094939,
     "end_time": "2024-04-27T22:00:43.615159",
     "exception": false,
     "start_time": "2024-04-27T22:00:43.520220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HyperparameterOptimizer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            device,\n",
    "            subset_train_graphs,\n",
    "            subset_train_labels,\n",
    "            subset_val_graphs,\n",
    "            subset_val_labels,\n",
    "            num_trials,\n",
    "            num_epochs):\n",
    "        self.device = device\n",
    "        self.subset_train_graphs = subset_train_graphs\n",
    "        self.subset_train_labels = subset_train_labels\n",
    "        self.subset_val_graphs = subset_val_graphs\n",
    "        self.subset_val_labels = subset_val_labels\n",
    "        self.num_trials = num_trials\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def objective(self, trial):\n",
    "        # Suggest hyperparameters\n",
    "        n_steps = trial.suggest_int('n_steps', 1, 40)\n",
    "        hidden_dim = trial.suggest_int('hidden_dim', 74, 256)\n",
    "        lr = trial.suggest_float('lr', 1e-4, 1e-1, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [64, 128, 256, 512])\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "\n",
    "        # Create the model, optimizer, and loaders\n",
    "        model = GraphClsGGNN(\n",
    "            annotation_size=74,\n",
    "            hidden_dim=hidden_dim,\n",
    "            n_steps=n_steps,\n",
    "            n_etypes=1,\n",
    "            num_cls=2,\n",
    "            dropout_rate=dropout_rate).to(\n",
    "            self.device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        train_loader = GraphDataLoader(\n",
    "            list(\n",
    "                zip(\n",
    "                    self.subset_train_graphs,\n",
    "                    self.subset_train_labels)),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate,\n",
    "            num_workers=8)\n",
    "        val_loader = GraphDataLoader(\n",
    "            list(\n",
    "                zip(\n",
    "                    self.subset_val_graphs,\n",
    "                    self.subset_val_labels)),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=collate,\n",
    "            num_workers=8)\n",
    "\n",
    "        # Training loop with pruning\n",
    "        model.train()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            model.train()  # Ensure the model is in training mode\n",
    "            for batched_graph, labels in train_loader:\n",
    "                batched_graph, labels = batched_graph.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "                loss = criterion(logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()  # Switch to evaluation mode for validation\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batched_graph, labels in val_loader:\n",
    "                    batched_graph, labels = batched_graph.to(self.device), labels.to(self.device)\n",
    "                    logits = model(batched_graph, batched_graph.ndata['h'].float())\n",
    "                    loss = criterion(logits, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            trial.report(val_loss, epoch)\n",
    "\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\"Run the hyperparameter optimization.\"\"\"\n",
    "        study = optuna.create_study(direction='minimize', pruner=MedianPruner())\n",
    "        study.optimize(self.objective, n_trials=self.num_trials)\n",
    "\n",
    "        best_hyperparams = study.best_trial.params\n",
    "        with open(f'best_hyperparams_GatedGraphConv_{RUN_NUMBER}.json', 'w') as f:\n",
    "            json.dump(best_hyperparams, f)\n",
    "        print(f\"Best hyperparameters are {best_hyperparams}.\")\n",
    "        print(\"Best hyperparameters saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad686f",
   "metadata": {
    "papermill": {
     "duration": 0.031558,
     "end_time": "2024-04-27T22:00:43.780978",
     "exception": false,
     "start_time": "2024-04-27T22:00:43.749420",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Main Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "772e00bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:43.795629Z",
     "iopub.status.busy": "2024-04-27T22:00:43.795176Z",
     "iopub.status.idle": "2024-04-27T22:00:43.827920Z",
     "shell.execute_reply": "2024-04-27T22:00:43.826089Z"
    },
    "papermill": {
     "duration": 0.044198,
     "end_time": "2024-04-27T22:00:43.831378",
     "exception": false,
     "start_time": "2024-04-27T22:00:43.787180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e9c012",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce58e793",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:00:43.964233Z",
     "iopub.status.busy": "2024-04-27T22:00:43.963956Z",
     "iopub.status.idle": "2024-04-27T22:00:44.721639Z",
     "shell.execute_reply": "2024-04-27T22:00:44.719506Z"
    },
    "papermill": {
     "duration": 0.88606,
     "end_time": "2024-04-27T22:00:44.724884",
     "exception": true,
     "start_time": "2024-04-27T22:00:43.838824",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "DGLError",
     "evalue": "File ../../data_mvi/graphs.bin does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDGLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load data and prepare for training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m graphs, labels_dict \u001b[38;5;241m=\u001b[39m \u001b[43mdgl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_graphs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../data_mvi/graphs.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Directly extract labels tensor from labels_dict\u001b[39;00m\n\u001b[1;32m      5\u001b[0m labels_tensor \u001b[38;5;241m=\u001b[39m labels_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/gnn/lib/python3.8/site-packages/dgl/data/graph_serialize.py:186\u001b[0m, in \u001b[0;36mload_graphs\u001b[0;34m(filename, idx_list)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load graphs and optionally their labels from file saved by :func:`save_graphs`.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03mBesides loading from local files, DGL supports loading the graphs directly\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03msave_graphs\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# if it is local file, do some sanity check\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m \u001b[43mcheck_local_file_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m version \u001b[38;5;241m=\u001b[39m _CAPI_GetFileVersion(filename)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gnn/lib/python3.8/site-packages/dgl/data/graph_serialize.py:40\u001b[0m, in \u001b[0;36mcheck_local_file_exists\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_local_file_exists\u001b[39m(filename):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_local_path(filename) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(filename):\n\u001b[0;32m---> 40\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m DGLError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(filename))\n",
      "\u001b[0;31mDGLError\u001b[0m: File ../../data_mvi/graphs.bin does not exist."
     ]
    }
   ],
   "source": [
    "# Load data and prepare for training\n",
    "graphs, labels_dict = dgl.load_graphs(\"../../data_mvi/graphs.bin\")\n",
    "\n",
    "# Directly extract labels tensor from labels_dict\n",
    "labels_tensor = labels_dict['labels']\n",
    "labels_tensor = labels_tensor.squeeze()\n",
    "\n",
    "# Convert tensor to numpy array for compatibility with sklearn\n",
    "labels_numpy = labels_tensor.numpy()\n",
    "\n",
    "# Split dataset train, test\n",
    "train_indices, test_indices, train_labels, test_labels = train_test_split(\n",
    "    range(labels_dict['labels'].size(0)), labels_numpy, test_size=0.2, stratify=labels_numpy,\n",
    "    random_state=RANDOM_STATE)\n",
    "\n",
    "# Split dataset train, validation\n",
    "train_indices, val_indices, train_labels, val_labels = train_test_split(\n",
    "    train_indices, train_labels, test_size=0.2, stratify=train_labels,\n",
    "    random_state=RANDOM_STATE)\n",
    "\n",
    "train_graphs = [graphs[i] for i in train_indices]\n",
    "test_graphs = [graphs[i] for i in test_indices]\n",
    "val_graphs = [graphs[i] for i in val_indices]\n",
    "\n",
    "subset_train_indices = np.random.choice(\n",
    "    len(train_graphs), size=int(len(train_graphs) * 0.2), replace=False)\n",
    "subset_train_graphs = [train_graphs[i] for i in subset_train_indices]\n",
    "subset_train_labels = train_labels[subset_train_indices]\n",
    "\n",
    "subset_val_indices = np.random.choice(\n",
    "    len(val_graphs), size=int(len(val_graphs) * 0.2), replace=False)\n",
    "subset_val_graphs = [val_graphs[i] for i in subset_val_indices]\n",
    "subset_val_labels = val_labels[subset_val_indices]\n",
    "\n",
    "# Combine train and validation graphs and labels for retraining\n",
    "combined_train_graphs = train_graphs + val_graphs\n",
    "combined_train_labels = np.concatenate((train_labels, val_labels))\n",
    "\n",
    "# annouce the start of the project\n",
    "print(\"Starting the project...\")\n",
    "print(\"\")\n",
    "\n",
    "# annouce the start of the data loading\n",
    "print(\"Starting data loading...\")\n",
    "print(\n",
    "    f'Train: {len(train_graphs)}, Validation: {len(val_graphs)}, '\n",
    "    f'Test: {len(test_graphs)}, \\nSubset Train: {len(subset_train_graphs)}, '\n",
    "    f'Subset Val: {len(subset_val_graphs)}'\n",
    ")\n",
    "print(\"\")\n",
    "print(\"Completed data loading.\")\n",
    "print(\"\")\n",
    "sys.stdout.flush()  # Force flushing of the buffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21877b95",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Perform Hyperparameter Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa21f3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T12:39:17.680895Z",
     "iopub.status.busy": "2024-04-22T12:39:17.680583Z",
     "iopub.status.idle": "2024-04-22T13:19:28.361922Z",
     "shell.execute_reply": "2024-04-22T13:19:28.359983Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Starting hyperparameter optimization...\")\n",
    "sys.stdout.flush()\n",
    "print(\"\")\n",
    "\n",
    "# Specify the number of trials and epochs for hyperparameter optimization\n",
    "optimizer = HyperparameterOptimizer(\n",
    "    device,\n",
    "    subset_train_graphs,\n",
    "    subset_train_labels,\n",
    "    subset_val_graphs,\n",
    "    subset_val_labels,\n",
    "    num_trials=20,\n",
    "    num_epochs=30)\n",
    "optimizer.optimize()\n",
    "print(\"Completed hyperparameter optimization.\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43480b16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T13:19:28.441521Z",
     "iopub.status.busy": "2024-04-22T13:19:28.440912Z",
     "iopub.status.idle": "2024-04-22T13:19:29.196075Z",
     "shell.execute_reply": "2024-04-22T13:19:29.194001Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'best_hyperparams_GatedGraphConv_{RUN_NUMBER}.json', 'r') as f:\n",
    "    best_hyperparams = json.load(f)\n",
    "\n",
    "# Correcting the use of best_hyperparams by\n",
    "train_loader = GraphDataLoader(list(zip(train_graphs,\n",
    "                                        train_labels)),\n",
    "                                batch_size=best_hyperparams['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=collate,\n",
    "                                num_workers=8)\n",
    "val_loader = GraphDataLoader(list(zip(val_graphs,\n",
    "                                        val_labels)),\n",
    "                                batch_size=best_hyperparams['batch_size'],\n",
    "                                shuffle=False,\n",
    "                                collate_fn=collate,\n",
    "                                num_workers=8)\n",
    "test_loader = GraphDataLoader(list(zip(test_graphs,\n",
    "                                        test_labels)),\n",
    "                                batch_size=best_hyperparams['batch_size'],\n",
    "                                shuffle=False,\n",
    "                                collate_fn=collate,\n",
    "                                num_workers=8)\n",
    "combined_train_loader = GraphDataLoader(\n",
    "    list(\n",
    "        zip(\n",
    "            combined_train_graphs,\n",
    "            combined_train_labels)),\n",
    "    batch_size=best_hyperparams['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate,\n",
    "    num_workers=8)\n",
    "print(\"Data loaders created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a5adc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T13:19:29.337170Z",
     "iopub.status.busy": "2024-04-22T13:19:29.336839Z",
     "iopub.status.idle": "2024-04-22T13:19:49.401867Z",
     "shell.execute_reply": "2024-04-22T13:19:49.399527Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Retraining with best hyperparameters...\")\n",
    "model = GraphClsGGNN(\n",
    "    annotation_size=74,\n",
    "    hidden_dim=best_hyperparams['hidden_dim'],\n",
    "    n_steps=best_hyperparams['n_steps'],\n",
    "    n_etypes=1,\n",
    "    num_cls=2,\n",
    "    dropout_rate=best_hyperparams['dropout_rate']).to(device)\n",
    "print(\"\")\n",
    "\n",
    "# Reset model parameters before retraining\n",
    "model.reset_parameters()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_hyperparams['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "early_stopping = EarlyStopping(patience_loss=10, patience_accuracy=10, \n",
    "                               verbose=False, delta_loss=0.001, \n",
    "                               delta_accuracy=0.001, path='checkpoint.pt', \n",
    "                               print_freq=5)\n",
    "\n",
    "training_pipeline = TrainingPipeline(device)\n",
    "training_pipeline.train_and_evaluate(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    early_stopping,\n",
    "    300,\n",
    "    plot_curves=True)\n",
    "optimal_epoch = early_stopping.best_epoch\n",
    "\n",
    "\n",
    "print(\"Completed training.\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e64e4f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Train Model with Best Hyperparameters on whole train set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a8e5d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Before final training on the combined train and val dataset, reset the\n",
    "# model again\n",
    "model.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ee09b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Final training on the combined train and val dataset\n",
    "print(\"Final training on the combined train and val dataset...\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_hyperparams['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "training_pipeline.train_and_evaluate(\n",
    "    model,\n",
    "    combined_train_loader,\n",
    "    None,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    None,\n",
    "    optimal_epoch,\n",
    "    plot_curves=False)\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), f'gatedgraph_model_run_{RUN_NUMBER}.pt')\n",
    "\n",
    "print(\"Completed training.\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f83f331",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Evaluate the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f8054c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation on the test set\n",
    "print(\"Evaluating on the test set...\")\n",
    "training_pipeline.evaluate_on_test(model, test_loader, criterion, RUN_NUMBER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.32436,
   "end_time": "2024-04-27T22:00:47.981001",
   "environment_variables": {},
   "exception": true,
   "input_path": "/home/xfulop/mvi/diploma_project_gnn/gatedgraphconv/gatedgraphconv_cls_run_2.ipynb",
   "output_path": "/home/xfulop/mvi/diploma_project_gnn/gatedgraphconv/gatedgraphconv_cls_run_2.ipynb",
   "parameters": {},
   "start_time": "2024-04-27T22:00:37.656641",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}